In this chapter we will document our prototype language, Join.  The
language is very similar to the original core join-calculus in many
aspects, but has been extended with various syntactic and semantic
features. In the following sections, we will present each extension
and discuss our motivation for including it, as well as the possible
implications that it may have on the difficulty of implementing the
language for real-world applications.

At last we will document the final syntax and semantics of the Join
language language.


\section{Pattern matching}

\section{Distribution and mobility}

\section{Timing}

The core join-calculus has no notion of time, which makes it
impossible to reason about the behaviour of programs with real-time
constraints.  For instance, in distributed systems, it is common to
place a time constraint on external requests, to make sure that a
program don't wait for a response forever if a message should get
lost, or if the program in the other end crashes. To express a
constraint like that in the core join calculus would require that we
rely on a specific implementation being able to generate a message on
a given time interval:
\begin{align*}
  \textbf{def}\quad & \S k<x> ~|~ \S incall<> \triangleright P_{ok} \\
  \land\quad & \S timeout<> ~|~ \S incall<> \triangleright P_{error} \\
  \textbf{in}\quad & \S remotecall<k> ~|~ \S starttimer<timeout, 10>
                                      ~|~ \S incall<>
\end{align*}
In the example above, \emph{remotecall} is given $10$ time units to
return a result on the name \emph{k}. If \emph{k} makes it before the
time limit, the process $P_{ok}$ is started, otherwise the process
$P_{error}$ starts.  There is a problem with this approach though:
Even though we can provide a Join-implementation that does exactly as
described, there is no guarantee that the program will behave in a
similar way on different implementations.

The problem lies both in the non-deterministic choice between the two
reaction rules, and in the fact that messages are not required to be
processed in the same order as they arrive. Even if a result arrives
before the timeout fires, a valid implementation can choose to wait
for the \emph{timeout} message to appear, and consume that instead of
the \emph{k} message.

A possible solution to this problem is to extend the join-calculus
with a notion of time. Many other non-timed process calculi, including
CSP and CCS, has already been extended for this purpose. An overview
of some of the work that has been done in this area along with an
attempt to generalize some of the concepts of timed process calculi
has been presented in \cite{nicollin-overview}.

A timed extension also exists for join-calculus, called Timed Join
Calculus \cite{timed-join}. The calculus is extended with a model of
time using a \emph{discrete time domain}, where every process bears a
time tag denoting when it will be able to participate in a reaction.
The syntax is extended with a new tagging construction for processes,
and all reactions are tagged with a
\emph{delay} tag:
\begin{align*}
  P ::={} & ...    & D ::={}& J \stackrel{d}{\triangleright} P \\
          & t : P  &        & D \land D
\end{align*}
We can now model the example above without depending on special
messages that get captured by the environment:
\begin{align*}
  \textbf{def}\quad & \S k<x> ~|~ \S incall<> \triangleright P_{ok} \\
  \land\quad & \S incall<> \stackrel{16}{\triangleright} P_{error} \\
  \textbf{in}\quad & \S 0:remotecall<k> ~|~ \S 0:incall<>
\end{align*}
Here, we assume that the response message \emph{k} gets transferred
from an external location and gets time tagged as soon as it enters
the local solution.  The second reaction rule is tagged with a delay
of $16$ time units.  This means that every message on the left of
``$\triangleright$'' needs to be available for $16$ time units before
the reaction can happen, effectively allowing another reaction to
``steal'' messages in that time window. Rules with no tags implicitly
gets tagged with a delay of $0$. If the \emph{k} message therefore
arrives before the $16$ time units has passed, the first reaction can
take place immediately.


\subsection{Operational semantics}

\fixme{Is it necessary to repeat the operational semantics of the
article, or can we just refer to them?}


\subsection{Infinite instants}

The time domain in Timed Join Calculus is \emph{abstract}, where time
tags doesn't have any quantifiable correspondence with \emph{physical}
time. It is assumed that any computation takes zero time unless
delayed with a non-zero time tag, which is of course not a realistic
assumption, but as assumption that simplifies the model.
Alternatively, one could assume that any atomic computation took some
minimum amount of time, yielding a model that is closer to reality.
However, as argued in \cite{nicollin-overview}, this destroys the
generality of the model, as we will then tie the behaviour of programs
to an arbitrary assumption about execution speed.

Even if we did make very conservative choices for the minimum duration
of a single computation, this wouldn't make it possible to guarantee
that a given computation finishes in a well-defined time window: Since
a computer has finite computational resources, but can (in theory)
execute an arbitrary number of threads concurrently, the time a
computation takes isn't fixed. As an example, say that we choose that
the duration $\delta$ of a reaction in the CHAM is $1 ms$. For a
single process, this will enable us to guarantee that a computation
involving 20 reactions will finish in $20 ms$. But if we execute an
arbitrary number of instances of this process concurrently on the same
hardware, we can only expect that the time it takes for all the
processes to finish will be proportional to the number of processes.
The actual wall-clock time for a single time step therefore increases,
meaning our hypothetical (though conservative) choice is still too
low. If we increase the number of concurrent processes towards
infinity, even the most conservative choice for $\delta$ will result
in an assumption that we can execute an arbitrary number of reactions
in an instant. The result is a more complicated model with exactly the
same problems as the simpler model, where every computation is assumed
to be instantaneous.

Assuming that every atomic computation takes zero time can pose some
problems in the form of \emph{timelocks} (also called
\emph{Zeno-behaviour}). Since time can only progress when a
computation in an instant is done, a diverging computation can prevent
time from ever progressing, causing a global timelock.  In
\cite{timed-join} this is solved by adding a very restrictive type
system to the join calculus, which rejects all programs that aren't
guaranteed to let time progress. We have chosen not to study this type
system in detail, for several reasons: (1) Identifying all
non-timelocking programs is equivalent to solving the halting problem,
meaning that the set of accepted programs in the proposed type system
is a lot smaller than the actual set of valid programs, making it very
difficult to express useful behaviour. (2) The authors describe the
type system as a ``first attempt'' at solving the problem, and are yet
to prove the soundness of it.


\section{Language definition}

\subsection{Syntax}

The syntax of our prototype language is defined in figure \ref{fig:syntax}. In
this definition, $x,a$ range over names, $s$ range over strings of characters,
and $i,d,t \in \mathbb{N}_0$. All constructions marked with ``*'' is syntactic
sugar, and has equivalent encodings in the core Join language.

\begin{figure}
\newcommand{\alt}{\mid\kern-1pt\mid}
\begin{align*}
P,Q\quad::={}&             && \textbf{processes} \\
          & 0              && \quad\textrm{inert process} \\
 \alt\quad& x\langle e_1, e_2, ..., e_n \rangle
                           && \quad\textrm{asynchronous message} \\
 \alt\quad& P~\&~Q         && \quad\textrm{parallel composition} \\
 \alt\quad& \textbf{def}~D~\textbf{in}~P
                           && \quad\textrm{local definition} \\
 \alt\quad& t : P          && \quad\textrm{timed process} \\
 \alt\quad& \textbf{match}~e~\textbf{with}~\pi_1
               \rightarrow P_1 ~|~ ... ~|~ \pi_n \rightarrow P_n
                           && \quad\textrm{pattern matching} \\
 \alt\quad& \{ I_1; I_2; ...; I_n \}
                           && \quad\textrm{*instruction sequence} \\
D\quad ::={}&              && \textbf{join definitions} \\
         & J \stackrel{d}{\triangleright} P
                           && \quad\textrm{delayed reaction} \\
\alt\quad& D~\textbf{or}~D && \quad\textrm{disjunction} \\
\alt\quad& a[D~\textbf{in}~P] && \quad\textrm{sublocation} \\
J\quad ::={}&              && \textbf{join patterns}\\
         & x\langle \pi_1, \pi_2, ..., \pi_n \rangle
                           && \quad\textrm{message} \\
\alt\quad& x(\pi_1, \pi_2, ..., \pi_n)
                           && \quad\textrm{*synchronous message} \\
\alt\quad& J~\&~J          && \quad\textrm{synchronization} \\
\pi\quad ::={}&            && \textbf{algebraic patterns} \\
         & x               && \quad\textrm{variable} \\
\alt\quad& \kappa(\pi_1, \pi_2, ..., \pi_n)
                           && \quad\textrm{constructor pattern} \\
e\quad ::={}&              && \textbf{expressions} \\
         & i               && \quad\textrm{*integer} \\
\alt\quad& " s "         && \quad\textrm{*string} \\
\alt\quad& x               && \quad\textrm{variable} \\
\alt\quad& \kappa(e_1, e_2, ..., e_n)
                           && \quad\textrm{constructor expression} \\
e^\star\quad ::={}&         && \textbf{*sugared expressions} \\
         & i               && \quad\textrm{*integer} \\
\alt\quad& " s "         && \quad\textrm{*string} \\
\alt\quad& x               && \quad\textrm{*variable} \\
\alt\quad& \kappa(e^\star_1, e^\star_2, ..., e^\star_n)
                           && \quad\textrm{*constructor expression} \\
\alt\quad& x(e^\star_1, e^\star_2, ..., e^\star_ n)
                           && \quad\textrm{*synchronous call} \\
I\quad ::={}&              && \textbf{*instructions} \\
         & \mathbf{let}~\pi~=~e^\star
                           && \quad\textrm{*named values} \\
\alt\quad& \mathbf{run}~P  && \quad\textrm{*asynchronous process} \\
\alt\quad& \mathbf{do}~e^\star && \quad\textrm{*synchronous call} \\
\alt\quad& \textbf{match}~e^\star~\textbf{with}~\pi_1
               \rightarrow \{ I* \} ~|~ ... ~|~ \pi_n \rightarrow \{ I* \}
                           && \quad\textrm{*pattern match} \\
\alt\quad& \textbf{return}~e^\star~\textbf{to}~x
                           && \quad\textrm{*implicit continuation}
\end{align*}
\caption{Syntax of the Join language.\label{fig:syntax}}
\end{figure}


\subsection{Semantics}

We model the state of our system as a series of CHAMs composed in parallel. A
CHAM is a four tuple $\mathcal{M} = (\mathcal{D}, \mathcal{A}, \phi, t)$
denoted by $\mathcal{D} \vdash^\phi_t \mathcal{A}$, having active definitions
$\mathcal{D}$, running processes $\mathcal{A}$, location string $\phi$ and time
$t$. Multiple CHAMs are composed in parallel using the associative-commutative
operator $\parallel$.

We model locality explicitly using the following rule, which says that a
reduction involving only a subset of a machine can be performed without
affecting the state of the rest of the machine:
\begin{align*}
\inferrule[Red-Loc]
{ \mathcal{M} \equiv \mathcal{D}_1 \vdash_t^\phi \mathcal{A}_1
\\ \mathcal{M}' \equiv \mathcal{D}_1 \vdash_t^\phi \mathcal{A}_1'
\\ \mathcal{M} \longrightarrow \mathcal{M}'}
{ \mathcal{D}_2,\mathcal{D}_1 \vdash_t^\phi \mathcal{A}_1,\mathcal{A}_2
\longrightarrow \mathcal{D}_2,\mathcal{D}_1 \vdash_t^\phi \mathcal{A}_1',\mathcal{A}_2
}
\end{align*}

We include a relaxed version of the time semantics from \cite{timed-join}. Our
version provides less guarantees about the order in which messages on the same
port is processed, and each CHAM is explicitly tagged with a natural number
that denotes the current time.

We begin by inductively defining a ternary relation $\cdot
\stackrel{\cdot}{\propto} \cdot$, which collects all the ready molecules in a
CHAM, and tags them with the time they became ready:
\begin{equation*}
\inferrule[Sched-Msg]
{ \mathcal{M} \equiv \mathcal{D} \vdash_t^\phi \mathcal{A}', t' : x\langle \tilde u \rangle
\\ t' \leq t
}
{\mathcal{M} \stackrel{t'}{\propto} t':x\langle \tilde u \rangle}
\end{equation*}

\begin{equation*}
\inferrule[Sched-Par]
{\mathcal{M} \stackrel{t}{\propto} P
\\ \mathcal{M} \stackrel{t}{\propto} Q}
{ \mathcal{M} \stackrel{max(t,t')}{\propto} P~\&~Q }
\end{equation*}
Using the collector, we can define reduction using the following rule:
\begin{equation*}
\inferrule[Reaction]
{
\mathcal{M} \equiv J \stackrel{d}{\triangleright} Q \vdash_t^\phi P
\hva\\ \mathcal{M}' \equiv J \stackrel{d}{\triangleright} Q \vdash_t^\phi t : \sigma_{rv}Q
\\ \mathcal{M} \stackrel{t'}{\propto} P
\\ t'+d = t
\\ \mathcal{P}(P) = \sigma_{rv}J }
{ \mathcal{M} \longrightarrow
  \mathcal{M}' }
\end{equation*}
In \textsc{Reaction}, $\sigma_{rv}$ replaces received variables with closed
expressions.

Prioritized pattern matching is performed using the following rule:
\begin{align*}
\inferrule[Match]
{\mathcal{M} \equiv~\vdash_t^\phi \textbf{match}~P~\textbf{with}~\pi_1
   \rightarrow Q_1 ~|~...~|~\pi_m \rightarrow Q_m
 \\ \mathcal{M}' \equiv~\vdash_t^\phi \eta_{rv}Q_i
 \\ \mathcal{P}(P) = \eta_{rv}\pi_i
 \\ \forall j < i.~\pi_j \not\preceq \eta_{rv} \pi_i}
{\mathcal{M} \longrightarrow \mathcal{M}'}
\end{align*}

To model unreliable communication between locations, we introduce the
$\diamond$ relation, which relates pairs of location names at specific times.
For example, $a \stackrel{t}{\diamond} b$ means that locations $a$ and $b$ can
communicate at time $t$. The relation is commutative. Using this, we can
specify how the system behaves when the connection between two units
disappear:
\begin{equation*}
\inferrule[Comm-Link]
{
\mathcal{C} \equiv \mathcal{D}_1 \vdash^{a\phi}_t \mathcal{A}_1, t:x\langle
\tilde e \rangle ~||~ \mathcal{D}_2 \vdash^{b\psi}_t \mathcal{A}_2
\hva\\ \mathcal{C}' \equiv \mathcal{D}_1 \vdash^{a\phi}_t \mathcal{A}_1 ~||~
\mathcal{D}_2 \vdash^{b\psi}_t \mathcal{A}_2, (t+1) : x \langle \tilde e
\rangle
\hva\\\\ x \in dv(\mathcal{D}_2)
\\ a \stackrel{t}{\diamond} b}
{ \mathcal{C} ~\longrightarrow~ \mathcal{C}' }
\end{equation*}
\begin{equation*}
\inferrule[Comm-Lost]
{
\mathcal{C} \equiv \mathcal{D}_1 \vdash^{a\phi}_t \mathcal{A}_1, t:x\langle
\tilde e \rangle ~||~ \mathcal{D}_2 \vdash^{b\psi}_t \mathcal{A}_2
\\
\mathcal{C}' \equiv \mathcal{D}_1 \vdash^{a\phi}_t \mathcal{A}_1 ~||~
\mathcal{D}_2 \vdash^{b\psi}_t \mathcal{A}_2
\hva\\\\
x \in dv(\mathcal{D}_2)
  \\ \neg(a \stackrel{t}{\diamond} b)}
{ \mathcal{C} ~\longrightarrow~ \mathcal{C}'}
\end{equation*}
Note that the above rules also state that communication can only take place
when the time of the two communicating machines are synchronised.
\fixme{We still need to specify that communication can't take place when one of
the locations are failed.}

In our system, time can only progress when no CHAM can take another step,
effectively synchronising all machines on a global clock. In a real setting,
this is of course not feasible, but it makes it easier to reason about
communicating devices in a simulated setting.

To be able to detect when a machine can't do any computations, we need to
ensure that the rules \textsc{Reaction} and \textsc{Match} cannot be applied.

%\begin{equation*}
%\inferrule[Par-Loc]
%{\mathcal{C} \equiv \mathcal{M}_1 ~||~ \mathcal{M}_2 \\
% \mathcal{C}' \equiv \mathcal{M}_1' ~||~ \mathcal{M}_2 \\
% \mathcal{M}_1 \longrightarrow \mathcal{M}_1'}
%{\mathcal{C} \longrightarrow \mathcal{C}'}
%\end{equation*}
