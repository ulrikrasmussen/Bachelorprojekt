% vim:spell:spelllang=en_gb:
% vim:set textwidth=80 fo+=tcroql:

In this chapter we will document our prototype language, Join.  The
language is very similar to the original core join-calculus in many
aspects, but has been extended with various syntactic and semantic
features.

In the first section, we will introduce the basics behind the core
join-calculus.

In the next section, we discuss the various extensions that has been
developed, our motivation for including them, and any eventual compromises we
have made.

In the last two sections, we document the syntax and semantics of our prototype
language, Join.

\section{The core join-calculus}

In this section, we will briefly describe the core join-calculus. We will
primarily rely on describing the semantics by example to give an overview.
Since our own language, Join, builds on a heavily modified version of the core
join-calculus, we will direct most of our attention towards documenting the
semantics of that. A more detailed definition of the join-calculus can be found
in \cite{fournet1996reflexive}.

The core calculus is based on the \emph{Reflexive Chemical Abstract Machine}
abbreviated RCHAM, which is a modification of the basic CHAM, defined by Berry
and Boudol in \cite{berry1989chemical}.

The model of computation in the RCHAM revolves around the concepts of
\emph{reaction rules} that manipulate \emph{atoms and molecules} in a
\emph{solution}. Solutions comprise two multisets $\D$ and $\A$, containing
reaction rules and processes, respectively. Solutions are denoted by $\D \vdash
\A$Â·

An atom is a message $\atm x<y>$, and a molecule is one or more
messages combined using the operator $\&$. Writing $\atm x<y>$, means that we
transmit the value $y$ on the name $x$. The only values in the calculus are
names, and they can both be used as port names and transmitted values.

As atoms enter the solution they recombine until a reaction rule is matched,
after which the left hand side of the matched rule is replaced with the right
hand side. The structural equivalence $\rightleftharpoons$ relates solutions
that haven't reacted yet, defining how atoms are allowed to ``mix and match'':
\begin{align*}
                  & \emptyset \vdash \atm x<a>, \atm y<b>, \atm z<c> \\
\rightleftharpoons{}& \emptyset \vdash \atm x<a> ~\& ~\atm y<b>, \atm z<c>
\end{align*}

The join-calculus builds on this model and defines the concepts of processes
$P$, join patterns $J$ and definitions $D$, which make it up respectively for
the atoms and reaction rules of the RCHAM. The abstract syntax of these
constructs are given below. $x,y$ range over names, and $\tilde y$ denotes a
vector of (zero or more) names:
\begin{displaymath}
\begin{array}{rl}
  P ::=& \atm x < \tilde y > \\
 |& \defJ D \inJ P\\
 |& P \andJ P
\end{array}\qquad
\begin{array}{rc}
  J ::=& \atm x < \tilde y > \\
 |& J \andJ J
\end{array}\qquad
\begin{array}{rc}
  D =& D \orJ D \\
 |& J \toJ{} P
\end{array}
\end{displaymath}

A reaction $J \triangleright P$ consumes molecules that match the pattern $J$,
and emits copies of the process $P$, where free names in $P$ have been
substituted for received names in $J$. Consider for instance the following
example, modelling a printing server that manages exclusive access to a printer.
The reaction rule $D$ is defined as $D = \atm ready<printer> ~\&~ \atm job<file>
\triangleright \atm printer<file>$:
\begin{align*}
  & D \vdash \atm ready<laser>, \atm job<1>, \atm job<2> \\
\rightleftharpoons{}& D \vdash \atm ready<laser> ~\&~ \atm job<2>, \atm job<1> \\
\longrightarrow{}& D \vdash \atm laser<2>, \atm job<1>
\end{align*}

Notice that in the above, non-determinism comes from $\rightleftharpoons$.

The RCHAM model is \emph{reflexive}, which means that a reaction can extend the
solution with new rules, making the model computationally complete. The molecule
$\mathbf{def}~D~\mathbf{in}~P$ can be split in two parts: A new set of
definitions $D$, and a set of new molecules $P$. The defining molecule can be
activated under the $\rightleftharpoons$ relation:
\begin{align*}
 & \emptyset \vdash \mathbf{def}~D~\mathbf{in}~P, \atm x<> \\
\rightleftharpoons{}& D \vdash P, \atm x<>
\end{align*}

For a defining molecule $\mathbf{def}~D~\mathbf{in}~P$, every port name that
occurs in the join patterns of $D$ is \emph{locally} bound in $P$. This means
that we can never extend or change the rule set for an existing port name, we
can only define new rules for new port names.

\subsection{Encodings}

As mentioned, the only values in the core join-calculus are names. However, the
calculus has been shown to encode the $\lambda$-calculus using a
CPS\footnote{Continuation Passing Style.} convention\cite{fournet1996reflexive}. It is
therefore possible (although not very practical) to represent values using
Church encodings or some similar scheme.

In the next section, we will discuss various extensions to the join-calculus,
one of which adds first-class values to the language.


\section{Extensions}

\subsection{Pattern matching}
As mentioned in the last section, the only values in the core join-calculus are
names. In \cite{MaMa2008AlgPat}, an extension of the syntax and semantics is
presented, which adds first-class algebraic data types with pattern matching to
the language.

Instead of being limited to only being able to transmit names in messages, the
join-calculus is extended to allow transmitting values of a small expression
language defined by the syntax
\begin{align*}
 e ::={}& x \\
  |\quad{}& \kappa(e_1, e_2, ..., e_n)
\end{align*}
where $n \geq 0$, $x$ range over names, and $\kappa$ range over data type
constructors. In examples, we will use the convention that algebraic
constructors begin with capitals, and names begin with lower case. Constructors
with zero arguments are written without parentheses ($Nil$ instead of $Nil()$).
A pattern language with syntax identical to the above can be used to specify
structural patterns inside join patterns. For example, a simple process
modelling a stack can be encoded as follows:
\begin{displaymath}
\begin{array}{rcll}
\defJ &&&
\\ & &   \atm stack<x> \andJ \atm push<y> &\toJ{} \atm stack<Cons(y,x)>
\\&\orJ&  \atm stack<Cons(x, xs)> \andJ \atm pop<k>& \toJ{} \atm stack<xs> \andJ \atm k<x>
\\&\inJ& \atm stack<Nil> \andJ \ldots
\end{array}
\end{displaymath}
In the above, $Cons$ and $Nil$ are constructors with $1$ and $0$ arguments,
respectively.

Apart from structural patterns in join patterns, a pattern matching process is
also added to the syntax, allowing the above to be expressed in the following
alternative way:
\begin{displaymath}
\begin{array}{rcll}
\defJ
\\ & &  \atm some<x> \andJ \atm push<y>&\toJ{} \atm some<Cons(y,x)>
\\ &\orJ&\atm empty<> \andJ \atm push<y> &\toJ{} \atm some<Cons(y,Nil)>
\\ &\orJ&\atm some<x> \andJ \atm pop<k> &\toJ{}  \match x \with
\\ &&& \qquad         Cons(x',Nil) \to \atm k<x> \andJ \atm empty<>
\\ &&& \qquad    |~ Cons(x',xs) \to \atm k<x> \andJ \atm some<xs>
\\&\inJ& \atm empty<> \andJ \ldots
\end{array}
\end{displaymath}

\subsubsection{Limitations of pattern matching}

We have chosen not to allow full structural pattern matching in join patterns,
as this complicates pattern matching on the same port names. Consider for
instance the following example:
\begin{equation*}
 \mathbf{def}~ \atm x<y>~\&~ \atm x<Two> \triangleright Q~
 \mathbf{in}~\atm x<Two>~\&~\atm x<Three>
\end{equation*}
The first join pattern is more general than the second one, and can match both
messages in the solution. The second join pattern can only match one of the
messages. If the more general pattern matches the message $\atm x<Two>$, the
second pattern won't be able to find a matching message in the solution. In
order to determine whether the reaction can trigger, we would have to try all
possible orderings of messages. To simplify the semantics (and implementation!),
we have therefore chosen to only allow structural pattern matching in
$\mathbf{match}$-expressions, where it is always well-defined what to do,
because we only match one value at a time, and don't have to take commutativity
and associativity of messages into account.


\subsection{Distribution and mobility}

In \cite{fournet1996calculus}, the join-calculus is extended with explicit
locations and primitives for mobility, allowing it to express mobile agents
moving between physical sites.

The model of distribution is based on the concept of \emph{locations}, which
are named solutions of definitions and atoms, thus comprising separate
join-calculus machines. A location resides on a physical site, and can move
atomically to other sites. Locations can contain nested sublocations, and are
thus organised in a tree. If a parent location migrates to a different site, it
takes all of its sublocations with it.

Locations consists of names $a,b,... \in \mathcal{L}$, where $\mathcal{L}$ is a
set of names. Location names can be organised in finite ordered sequences,
$\phi, \psi, ... \in \mathcal{L}^*$. A mobile agent is a labelled solution $\D
\vdash^\phi \A$. And agent $\vdash^\phi$ is running in a sublocation of
$\vdash^\psi$ if $\phi$ is a prefix of $\psi$. The entire system of running
agents is called the distributed reflexive chemical abstract machine (DRCHAM),
and consists of a multiset of labelled solutions. Every labelled solution is
uniquely identified by its rightmost location name. A DRCHAM is denoted by one
or more labelled solutions, separated by $||$.


\subsubsection{Communication}
Locally, each labelled solution works exactly like the core join-calculus, and
won't affect other solutions in the DRCHAM. However, if a solution emits a
message with a port name which is defined in a different solution in the DRCHAM,
the message is atomically transferred to that solution, allowing the two
solutions to communicate:
\begin{align*}
 &\emptyset \vdash^\phi \atm x<y> ~||~ \atm x<y> \triangleright Q \vdash^\psi \\
 \longrightarrow& \emptyset \vdash^\phi ~||~ \atm x<y> \triangleright Q
 \vdash^\psi \atm x<y>
\end{align*}

It is assumed that every name is defined in at most one solution, resulting in
all communication being deterministic and point-to-point.


\subsubsection{Mobility and failure}

The syntax of the join-calculus is extended to include a notion of defining new
sublocations for a given solution:
\begin{align*}
 D ::={}& ... ~|~ a[D : P]
\end{align*}

The $\rightleftharpoons$ defines how a defined sublocation can become a solution
in the DRCHAM (and vice-versa). Example:
\begin{align*}
 a[D : P] \vdash^\phi \rightleftharpoons \vdash^\phi ~||~ D \vdash^{\phi a} P
\end{align*}
When all sublocations for a given location has been reduced to definition form,
we say that the location is \emph{frozen}. A location can only transcend to
definition form when all of its sublocations are frozen.

The syntax of processes is also extended to include a special message, $go$,
which denotes that a location wants to migrate and become a sublocation of
another location. Messages for halting a location and detecting failure is also
added:
\begin{align*}
 P ::={}& ... ~|~ \atm go<a,k> ~|~ \atm halt<> ~|~ \atm fail<a,k>
\end{align*}

If a solution emits message of the form $\atm go<a,k>$, it will become a
sublocation of the location $a$, and emit $k$ when the migration is done. The
migration is atomic, and can only take place when the migrating location is in
definition form (effectively taking all its sublocations with it in the
migration). Example:
\begin{align*}
 a[D : P ~\&~ \atm go<b,k>] \vdash^\phi ~||~ \vdash^{\psi b}
\longrightarrow \vdash^\phi ~||~ a[D : \atm k<>] \vdash^{\psi b}
\end{align*}

If a solution emits a message of the form $\atm halt<>$, it will be marked as
\emph{dead}. A dead solution is denoted by prefixing the location name with
$\Omega$. Halting a location is atomic can only take place when the location
being halted is in definition form (effectively also halting all of its
sublocations). Example:
\begin{align*}
  a[D : P ~\&~ \atm halt<>] \vdash^\phi \longrightarrow \Omega a[D : P] \vdash^\phi
\end{align*}
If a solution has a location string which contains $\Omega$, it is not allowed
to make any reactions. Other locations can still migrate to a failed location,
but will also get marked as failed in the same atomic action, preventing to
perform any further actions.

If a solution emits a message of the form $\atm fail<a,k>$, it can observe
when the location $a$ fails, by emitting a continuation on port $k$. Example:
\begin{align*}
 \vdash^\phi \atm fail<a, k> ~||~ \vdash^{\psi a} \longrightarrow \vdash^\phi
 \atm k<> ~||~ \vdash^{\psi a}
\end{align*}
where $\psi$ contains $\Omega$.

\subsubsection{Limitations}

In our language, we have chosen to include most of the extensions presented in
\cite{fournet1996calculus}. We have not chosen to include support for failure
detection. It is easy to implement failure detection in a simulated setting.
However, in a real-world distributed setting, it is unclear what behaviour the
programmer actually can expect.

As an example, consider a scenario where several observing locations have
emitted $fail$ atoms for a single location $a$. This poses (at least) the
following problems from an implementation point of view:
\begin{enumerate}
 \item If $a$ fails, the implementation effectively needs to perform a broadcast
 operation to notify every observer of the failure. If some of the observers
 have come out of reach at the time of failure they may not get notified, unless
 the implementation actively broadcasts the failure status until it is sure that
 everyone has been notified. First of all, this makes us run into the
 \emph{rendezvous} problem, since we can't determine when everyone have
 successfully been notified. Second, this kind of behaviour is not desirable in
 a distributed setting with embedded devices where we are interested in keeping
 the network overhead as low as possible.

 \item It will be up to the physical unit on which the failed location resides
 to answer all $fail$-probes. Since a failed location never disappears from the
 formalism, the physical unit may have to maintain a table of all the failed
 locations that resides on it. For memory and processor constrained devices,
 this is not desirable.

 \item Failure detection is timeout-agnostic. If the physical unit on which a
 given location resides comes out of reach, we cannot conclude failure, because
 we cannot guarantee that the given location won't reappear sometime in the
 future. If the physical unit blows up, the residing locations will effectively
 be dead indefinitely, but the rest of the system will never be able to observe
 it.
\end{enumerate}

For these reasons, we feel that the usefulness of failure detection as a
primitive is so limited that it is not worth investigating it in our use cases.
Using the existing communication primitives, the programmer should also be able
to implement behaviour equivalent of $fail$, but with the possibility of
specifying constraints like timeout.

\subsection{Time}

The core join-calculus has no notion of time, which makes it
impossible to reason about the behaviour of programs with real-time
constraints.  For instance, in distributed systems, it is common to
place a time constraint on external requests, to make sure that a
program don't wait for a response forever if a message should get
lost, or if the program in the other end crashes. To express a
constraint like that in the core join-calculus would require that we
rely on a specific implementation being able to generate a message on
a given time interval:
\begin{align*}
  \textbf{def}\quad & \atm k<x> ~|~ \atm incall<> \triangleright P_{ok} \\
  \land\quad & \atm timeout<> ~|~ \atm incall<> \triangleright P_{error} \\
  \textbf{in}\quad & \atm remotecall<k> ~|~ \atm starttimer<timeout, 10>
                                      ~|~ \atm incall<>
\end{align*}
In the example above, \emph{remotecall} is given $10$ time units to
return a result on the name \emph{k}. If \emph{k} makes it before the
time limit, the process $P_{ok}$ is started, otherwise the process
$P_{error}$ starts.  There is a problem with this approach though:
Even though we can provide a language implementation that does exactly as
described, there is no guarantee that the program will behave in a similar way
across implementations.

The problem lies both in the non-deterministic choice between the two
reaction rules, and in the fact that messages are not required to be
processed in the same order as they arrive. Even if a result arrives
before the timeout fires, a valid implementation can choose to wait
for the \emph{timeout} message to appear, and consume that instead of
the \emph{k} message.

A possible solution to this problem is to extend the join-calculus
with a notion of time. Many other non-timed process calculi, including
CSP and CCS, has already been extended for this purpose. An overview
of some of the work that has been done in this area along with an
attempt to generalize some of the concepts of timed process calculi
has been presented in \cite{nicollin-overview}.

A timed extension also exists for join-calculus, called Timed Join
Calculus \cite{timed-join}. The calculus is extended with a model of
time using a \emph{discrete time domain}, where every process bears a
time tag denoting when it will be able to participate in a reaction.
The syntax is extended with a new tagging construction for processes,
and all reactions are tagged with a
\emph{delay} tag:
\begin{align*}
  P ::={} & ...    & D ::={}& J \stackrel{d}{\triangleright} P \\
          & t : P  &        & D \land D
\end{align*}
We can now model the example above without depending on special
messages that get captured by the environment:
\begin{align*}
  \textbf{def}\quad & \atm k<x> ~|~ \atm incall<> \triangleright P_{ok} \\
  \land\quad & \atm incall<> \stackrel{16}{\triangleright} P_{error} \\
  \textbf{in}\quad & \atm 0:remotecall<k> ~|~ \atm 0:incall<>
\end{align*}
Here, we assume that the response message \emph{k} gets transferred
from an external location and gets time tagged as soon as it enters
the local solution.  The second reaction rule is tagged with a delay
of $16$ time units.  This means that every message on the left of
``$\triangleright$'' needs to be available for $16$ time units before
the reaction can happen, effectively allowing another reaction to
``steal'' messages in that time window. Rules with no tags implicitly
gets tagged with a delay of $0$. If the \emph{k} message therefore
arrives before the $16$ time units has passed, the first reaction can
take place immediately.


\subsubsection{Infinite instants}

The time domain in Timed Join Calculus is \emph{abstract}, where time
tags doesn't have any quantifiable correspondence with \emph{physical}
time. It is assumed that any computation takes zero time unless
delayed with a non-zero time tag, which is of course not a realistic
assumption, but an assumption that simplifies the model.
Alternatively, one could assume that any atomic computation took some
minimum amount of time, yielding a model that is closer to reality.
However, as argued in \cite{nicollin-overview}, this destroys the
generality of the model, as we would then tie the behaviour of programs
to an arbitrary assumption about execution speed.

Even if we did make very conservative choices for the minimum duration
of a single computation, this wouldn't make it possible to guarantee
that a given computation finishes in a well-defined time window: Since
a computer has finite computational resources, but can (in theory)
execute an arbitrary number of threads concurrently, the time a
computation takes isn't fixed. As an example, say that we choose that
the duration $\delta$ of a reaction in the CHAM is $1 ms$. For a
single process, this will enable us to guarantee that a computation
involving 20 reactions will finish in $20 ms$. But if we execute an
arbitrary number of instances of this process concurrently on the same
hardware, we can only expect that the time it takes for all the
processes to finish will be proportional to the number of processes.
The actual wall-clock time for a single time step therefore increases,
meaning our hypothetical (though conservative) choice is still too
low. If we increase the number of concurrent processes towards
infinity, even the most conservative choice for $\delta$ will result
in an assumption that we can execute an arbitrary number of reactions
in an instant. The result is a more complicated model with exactly the
same problems as the simpler model, where every computation is assumed
to be instantaneous.

Assuming that every atomic computation takes zero time can pose some
problems in the form of \emph{timelocks} (also called
\emph{Zeno-behaviour}). Since time can only progress when a
computation in an instant is done, a diverging computation can prevent
time from ever progressing, causing a global timelock.  In
\cite{timed-join} this is solved by adding a very restrictive type
system to the join-calculus, which rejects all programs that aren't
guaranteed to let time progress. We have chosen not to study this type
system in detail, for several reasons: (1) Identifying all
non-timelocking programs is equivalent to solving the halting problem,
meaning that the set of accepted programs in the proposed type system
is a lot smaller than the actual set of valid programs, making it very
difficult to express useful behaviour. (2) The authors describe the
type system as a ``first attempt'' at solving the problem, and are yet
to prove the soundness of it.


\section{Syntax}

The syntax of our prototype language is defined in Figure \ref{fig:syntax} and
\ref{fig:syntax2}. In this definition, $x,y,a$ range over names, $s$ ranges over
strings of characters, and $i,d,t \in \mathbb{N}_0$.

We inductively define a set of functions over the syntax which determine the
free variables, defined variables and received variables of a processes $P$,
definitions $D$ and join patterns $J$. The set of received variables of a join
pattern is simply the union of all the names that bind received values. We
require that join patterns are linear, meaning that each received variable may
occur at most once:
\begin{align*}
rv(\top) \defEq{}& \emptyset \\
rv(j~\&~J) \defEq{}& rv(j) \uplus rv(J) \\
rv(\atm x<y>) \defEq{}& \{y\}
\end{align*}
Received variables also appear in algebraic patterns:
\begin{align*}
rv(x) \defEq{}& \{x\} \\
rv(\kappa(\pi_1, \pi_2, ..., \pi_n)) \defEq{}& \bigcup_{i \leq n} \pi_i \\
\end{align*}

The set of defined variables for a definition is the set of all names that
appear as port names in join patterns:
\begin{align*}
 dv(\top) \defEq{}& \emptyset \\
 dv(j~\&~J) \defEq{}& dv(j) \cup dv(J) \\
\end{align*}
for definitions, we have:
\begin{align*}
 dv(J \stackrel{d}{\triangleright} Q) \defEq{}& dv(J) \\
 dv(a[D~\mathbf{in}~P]) \defEq{}& \{a\} \cup dv(D) \\
 dv(D_1~\mathbf{or}~D_2) \defEq{}& dv(D_1) \cup dv(D_2)
\end{align*}

The set of free variables for a process is the set of all names that are not
locally bound by any definitions. For processes, we have:
\begin{align*}
 fv(0) \defEq{}& \emptyset \\
 fv(\atm x<y>) \defEq{}& \{x,y\} \\
 fv(\mathbf{def}~D~\mathbf{in}~P) \defEq{}& (fv(P) \cup fv(D)) \setminus dv(D)
 \\
 fv(\mathbf{match}~e~\mathbf{with}~\Pi) \defEq{}& fv(e) \cup fv(\Pi) \\
 fv(d : P) \defEq{}& fv(P) \\
\end{align*}
for match patterns, we have:
\begin{align*}
 fv(\top) \defEq{}& \emptyset \\
 fv(\pi \rightarrow P~|~\Pi) \defEq{}& (fv(P) \setminus rv(\pi)) \cup fv(\Pi)
\end{align*}
for expressions, we have:
\begin{align*}
 fv(x) \defEq{}& \{x\} \\
 fv(\kappa(e_1, e_2, ..., e_n)) \defEq{}& \bigcup_{i \leq n} fv(e_i)
\end{align*}

Calculating variable sets over a set of syntax trees is just defined as the
union of the function applied to each element of the set:
\begin{align*}
 dv(\D) \defEq{}& \bigcup_{D \in \D} dv(D) \\
 fv(\D) \defEq{}& \bigcup_{D \in \D} fv(D) \\
 fv(\A) \defEq{}& \bigcup_{P \in \A} fv(P)
\end{align*}

\subsection{Syntactic sugar}
All constructions marked with ``*'' are syntactic sugar, and have equivalent
encodings in the core Join language. For instance, integers can be encoded as
Peano numbers using two data constructors, and strings are just lists of
integers.

The syntax of the language only supports messages and join patterns with one
value and variable name respectively. By using a convention for encoding tuples
as data constructors, we can provide syntactic sugar that supports sending more
than one value on a port name, like in the original join-calculus. For instance,
the message $\atm x<1, 12, 42>$ can be encoded as $\atm x<Tuple3(1, 12, 42)>$. In
join patterns, the sugared construction
\begin{equation*}
 \mathbf{def}~\atm x<a_1, a_2, a_3> \stackrel{d}{\triangleright} Q
\end{equation*}
can be trivially desugared to
\begin{equation*}
 \mathbf{def}~\atm x<y> \stackrel{d}{\triangleright} ~\mathbf{match}~ y
 ~\mathbf{with}~ Tuple3(a_1,a_2,a_3) \rightarrow Q
\end{equation*}

The language includes a sequential subset, where processes of the form $\{ I_1;
I_2;...;I_n\}$ consists of a series of instructions that gets executed
sequentially. This sequential subset of the language is desugared using a CPS to
enforce sequential evaluation, in the same way as it is done in
\cite{fournet1996reflexive}. We won't go into much detail as to how this sub
language is translated into the core Join language as the translation is pretty
straightforward.


\begin{figure}
\begin{align*}
P,Q\quad::={}&             && \textbf{processes} \\
          & 0              && \quad\textrm{inert process} \\
 \alt\quad& x\langle e \rangle
                           && \quad\textrm{asynchronous message} \\
 \alt\quad& P~\&~Q         && \quad\textrm{parallel composition} \\
 \alt\quad& \textbf{def}~D~\textbf{in}~P
                           && \quad\textrm{local definition} \\
 \alt\quad& d : P          && \quad\textrm{delayed process} \\
 \alt\quad& \textbf{match}~e~\textbf{with}~\Pi
                           && \quad\textrm{pattern matching} \\
 \alt\quad& \{ I_1; I_2; ...; I_n \}
                           && \quad\textrm{*instruction sequence} \\
\Pi\quad ::={}&            && \textbf{prioritised match alternatives} \\
 \alt\quad& \top           && \quad\textrm{empty alternative} \\
 \alt\quad& \pi \rightarrow P ~|~ \Pi
                           && \quad\textrm{match alternative} \\
D\quad ::={}&              && \textbf{join definitions} \\
         & J \stackrel{d}{\triangleright} P
                           && \quad\textrm{delayed reaction} \\
\alt\quad& D~\textbf{or}~D && \quad\textrm{disjunction} \\
\alt\quad& a[D~\textbf{in}~P] && \quad\textrm{sublocation} \\
j\quad ::={}&             && \textbf{join pattern} \\
            & x\langle y \rangle
                           && \quad\textrm{message} \\
            & x\langle y_1, y_2, ..., y_n \rangle
                           && \quad\textrm{*$n$-nary message} \\
\alt\quad& x(y_1, y_2, ..., y_n)
                           && \quad\textrm{*synchronous $n$-ary message} \\
J\quad ::={}&              && \textbf{synchronised join patterns}\\
\alt\quad& \top               && \quad\textrm{empty pattern}\\
\alt\quad& j ~\&~ J
                           && \quad\textrm{synchronization} \\
\pi\quad ::={}&            && \textbf{algebraic patterns} \\
         & x               && \quad\textrm{variable} \\
\alt\quad& \kappa(\pi_1, \pi_2, ..., \pi_n)
                           && \quad\textrm{constructor pattern} \\
e\quad ::={}&              && \textbf{expressions} \\
         & i               && \quad\textrm{*integer} \\
\alt\quad& " s "         && \quad\textrm{*string} \\
\alt\quad& x               && \quad\textrm{variable} \\
\alt\quad& \kappa(e_1, e_2, ..., e_n)
                           && \quad\textrm{constructor expression} \\
\end{align*}
\caption{Syntax of the Join language (continued in Figure
\ref{fig:syntax2}).\label{fig:syntax}}
\end{figure}

\begin{figure}
\begin{align*}
e^\star\quad ::={}&         && \textbf{*sugared expressions} \\
         & i               && \quad\textrm{*integer} \\
\alt\quad& " s "         && \quad\textrm{*string} \\
\alt\quad& x               && \quad\textrm{*variable} \\
\alt\quad& \kappa(e^\star_1, e^\star_2, ..., e^\star_n)
                           && \quad\textrm{*constructor expression} \\
\alt\quad& x(e^\star_1, e^\star_2, ..., e^\star_ n)
                           && \quad\textrm{*synchronous call} \\
I\quad ::={}&              && \textbf{*instructions} \\
         & \mathbf{let}~\pi~=~e^\star
                           && \quad\textrm{*named values} \\
\alt\quad& \mathbf{run}~P  && \quad\textrm{*asynchronous process} \\
\alt\quad& \mathbf{do}~e^\star && \quad\textrm{*synchronous call} \\
\alt\quad& \textbf{match}~e^\star~\textbf{with}~\pi_1
               \rightarrow \{ I* \} ~|~ ... ~|~ \pi_n \rightarrow \{ I* \}
                           && \quad\textrm{*pattern match} \\
\alt\quad& \textbf{return}~e^\star~\textbf{to}~x
                           && \quad\textrm{*implicit continuation}
\end{align*}
\caption{Syntax of the Join language, continued.\label{fig:syntax2}}
\end{figure}



\section{Semantics}
Throughout this section, we will denote multisets using ``bag''-notation. For
example, the following is a multiset containing two $P_1$ processes and one
$P_2$ process:
\begin{equation*}
 \A = \Lbag \P_1, \P_1, \P_2 \Rbag
\end{equation*}
Multiset union is denoted by the $\uplus$ operator:
\begin{equation*}
 \A' = \A_1 \uplus \A_2
\end{equation*}

We model the state of our system as a series of CHAMs composed in parallel. A
single CHAM is a four tuple $\M = (\D, \A, \phi, t)$ denoted by
\begin{equation*}
 \M ::= \D \mscJ \A
\end{equation*}
where $\D$ is a set of active definitions, $\A$ is a multiset of messages,
$\phi$ is an ordered sequence of location names, and $t$ is a natural number
denoting the current time.

We restrict the possible forms of active definitions and messages, so that an
active message must follow the syntax
\begin{equation*}
 A ::= t : \atm x<v>
\end{equation*}
meaning that any multiset of active messages $\A$ has the form
\begin{equation*}
 \A ::= \Lbag A_1, A_2, ..., A_n \Rbag
\end{equation*}
Active definitions are restricted to the syntax
\begin{equation*}
 R ::= J \stackrel{d}{\triangleright} Q ~\big|~ a[D~\mathbf{in}~P]
\end{equation*}
meaning that any set of active definitions has the form
\begin{equation*}
 \D ::= \{ R_1, R_2, ..., R_n \}
\end{equation*}

Zero or more CHAMs can be composed in parallel, which
is modelled using a set:
\begin{align*}
 \C ::={}& \{ \M_1, \M_2, ..., \M_n \}
\end{align*}
The empty set $\emptyset$ is the totally inactive system.

Any location string $\phi$ has at least one element and follows the syntax
\begin{equation*}
 \phi ::= l~a_1 a_2 ... a_n
\end{equation*}
Where $l$ is a physical location name representing the physical machine on
which a given abstract machine is running. The path $a_1a_2...a_n$ consists of
zero or more location names denoting the sublocation path. For every system
$\C$, the following holds:
\begin{invariant}
 \label{thm:unique-loc}
 The last location name in a location string uniquely identifies an abstract
 machine:

 $\forall (\D_1 \vdash_{t_1}^{\phi_1 a} \A_1), (\D_2 \vdash_{t_2}^{\phi_2 b}
 \A_2) \in \C.~ a = b \implies \phi_1 = \phi_2$
\end{invariant}


\subsection{Time}
We include a relaxed version of the time semantics from \cite{timed-join}. Our
version of the semantics provides less guarantees about the order in which
messages are processed, but it still forces all computations at a given instant
$t$ to complete before computations involving messages with a delay tag larger
than $t$ can happen. Each CHAM is also explicitly tagged with a natural number
that denotes the current time. This explicit tagging does not occur in
\cite{timed-join}, but is added to these semantics because:
\begin{enumerate}
 \item It enables us to synchronise every machine in the system on a global
 clock. This is necessary to make progress of time deterministic, which makes
 it easier to reason about prototype programs written in Join.
 \item By having the current time available in all reduction rules, we can
 avoid having to generate all possible transitions from a given state and then
 choose the ``earliest''. It is not clear how an actual implementation should
 do this in a feasible manner.
\end{enumerate}

\subsection{Unreliable channels}
We make unreliable communication between locations an explicit part of the
semantics. This is done to make the model of unreliable communication
deterministic, making it easier to evaluate how well different revisions of a
program handles lost messages.

We introduce the $\diamond$ relation, which relates pairs of location names at
specific times. For example, when locations $a$ and $b$ can communicate at time
$t$, we denote it by
\begin{equation*}
 a \linkUp{t} b
\end{equation*}
The relation is reflexive, but not necessarily symmetric or transitive.


\subsection{Pattern matching}
We say that an algebraic pattern $\pi$ \emph{matches} a value $v$, when there
exists a $\sigma$ such that $\sigma \pi = v$, where $\sigma$ is a substitution
function that substitutes closed expressions for variable names.  When checking
whether a pattern $\pi$ matches an expression $v$, we denote it with
\begin{equation*}
  \boxed{\P(\pi,v) \leadsto \hat\sigma}
\end{equation*}
Where $\hat\sigma$ can denote that a pattern either matches or \emph{fails} to
match a process:
\begin{align*}
 \hat\sigma ::={}& \sigma \\
         |\quad{}& fail
\end{align*}

The inference rules for pattern matching are given in Figure \ref{fig:rule:pat}.

\begin{figure}
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\P(\pi, v) \leadsto \hat\sigma$}
\begin{equation*}
\inferrule[Pat-Var]
{~}
{\P(x,v) \leadsto (x \mapsto v)}
\end{equation*}
\begin{equation*}
\inferrule[Pat-Cons]
{\pi = \kappa(\pi_1, \pi_2, ..., \pi_n)
\\ v = \kappa(v_1, v_2, ..., v_n)
\\ \P(\pi_1,v_1) \leadsto \sigma_1
\\ \P(\pi_2,v_2) \leadsto \sigma_2
\\ ...
\\ \P(\pi_n,v_n) \leadsto \sigma_n}
{
 \P(\pi,v) \leadsto \sigma_1 \cup \sigma_2 \cup ... \cup \sigma_n
}
\end{equation*}
\begin{equation*}
\inferrule[Pat-Cons-Fail]
{\pi = \kappa(\tilde\pi)
\\ v = \kappa'(\tilde v)
\\ \kappa \not= \kappa'
}
{
 \P(\pi, v) \leadsto fail
}
\end{equation*}
\begin{equation*}
\inferrule[Pat-Cons-SubFail]
{\pi = \kappa(\pi_1, \pi_2, ..., \pi_n)
\\ v = \kappa(v_1, v_2, ..., v_m)
\\ \P(\pi_i, v_i) \leadsto fail
}
{
 \P(\pi, v) \leadsto fail
}
\end{equation*}
\end{minipage}}
\caption{Inference rules for pattern matching.} \label{fig:rule:pat}
\end{figure}

\subsection{Heating}
In the original join-calculus definition, a set of reversible heating/cooling
rules is given. These rules are then implicitly applied to satisfy the
side-conditions of each reduction rule. This results in a system which is very
concisely described, but it doesn't give any direct hints as to when an actual
implementation should apply heating or cooling rules.

Our semantics is more verbose, but provides a directed set of rules which
explicitly specifies when to heat atoms. We have also excluded the reverse
relations, so cooling will never occur.

Heating of a definition construct $D$ into a set of single reaction rules and
sublocation definitions $\D'$ are denoted by the judgement form
\begin{equation*}
  \boxed{\H'(D) \leadsto \D'}
\end{equation*}

Heating of a processes $P$ into a definition set $\D'$ and a set of messages
$\A'$ is denoted by the judgement form
\begin{equation*}
  \boxed{\H(P) \leadsto (\D', \A')}
\end{equation*}
The inference rules for heating are given in Figure \ref{fig:rule:heat}.

\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\H'(D) \leadsto \D'$}
\begin{equation*}
\inferrule[Heat-Or]
{D = D_1~\mathbf{or}~D_2
\\ \H'(D_1) \leadsto \D_1
\\ \H'(D_2) \leadsto \D_2}
{\H'(D) \leadsto \D_1 \cup \D_2}
\end{equation*}
\begin{equation*}
\inferrule[Heat-Reaction]
{D = J \stackrel{d}{\triangleright} Q}
{\H'(D) \leadsto \{ D \}}
\qquad
\inferrule[Heat-Location]
{D = a[D~\mathbf{in}~P]}
{\H'(D) \leadsto \{ D \}}
\end{equation*}
\doms{$\mathcal{H}(P) \leadsto (\D', \A')$}
\begin{equation*}
\inferrule[Heat-Msg]
{P = \atm d:x<v>}
{\H(P) \leadsto (\emptyset, \Lbag P \Rbag)}
\qquad
\inferrule[Heat-Inert]
{P = d : 0}
{\H(P) \leadsto (\emptyset, \emptyset)}
\end{equation*}
\begin{equation*}
\inferrule[Heat-Par]
{P = d : (P_1~\&~P_2)
\\ \H(d:P_1) \leadsto (\D_1, \A_1)
\\ \H(d:P_2) \leadsto (\D_2, \A_2)
}
{\H(P) \leadsto (\D_1 \cup \D_2, \A_1 \uplus \A_2)}
\end{equation*}
\begin{equation*}
\inferrule[Heat-Time]
{P = d:(d':P)
\\ \H((d+d'):P) \leadsto (\D, \A)
}
{\H(P) \leadsto (\D,\A)}
\end{equation*}
\begin{equation*}
\inferrule[Heat-Def\footnote{Where $\sigma_{dv}$ replaces fresh names for defined variables.}]
{P = d:\mathbf{def}~D~\mathbf{in}~P
\\ \H'(\sigma_{dv} D) \leadsto \D_1
\\ \H(d: \sigma_{dv} P) \leadsto (\D_2, \A)
}
{\H(P) \leadsto (\D_1 \cup \D_2, \A)}
\end{equation*}
\begin{equation*}
\inferrule[Heat-Match]
{P = d:\mathbf{match}~e~\mathbf{with}~\pi \rightarrow Q~|~\Pi
\\ \P(\pi,e) \leadsto \sigma
}
{\H(P) \leadsto (\emptyset, \Lbag d : \sigma Q \Rbag)}
\end{equation*}
\begin{equation*}
\inferrule[Heat-Match-Next]
{P = d:\mathbf{match}~e~\mathbf{with}~\pi \rightarrow Q~|~\Pi
\\ \P(\pi,e) \leadsto fail
\\ P' = d:\mathbf{match}~e~\mathbf{with}~\Pi
\\ \H(P') \leadsto (\D, \A)
}
{\H(P) \leadsto (\D,\A)}
\end{equation*}
\begin{equation*}
\inferrule[Heat-NoMatch]
{P = d:\mathbf{match}~e~\mathbf{with}~\emptyset}
{\H(P) \leadsto (\emptyset, \Lbag \atm d:halt<> \Rbag) }
\end{equation*}
\end{minipage}}
\caption{Heating rules}\label{fig:rule:heat}
\end{figure}

%\clearpage

\subsection{Reactions}

When triggering a reaction in a local machine, we first need to determine if
all the required messages are present in the solution. Checking whether a
single join pattern (that is, a join pattern with no synchronisation) is
satisfied, is denoted by
\begin{equation*}
 \boxed{\P'(j,\A) \leadsto \widehat\T_\J}
\end{equation*}
where the syntax of $\widehat\T_\J$ can denote either success or failure:
\begin{align*}
 \widehat\T_\J ::= {}& (t, \sigma, \A') \\
       |\quad& fail
\end{align*}

This means that if we can conclude $\P'(j, \A) \leadsto (t,\sigma,\A')$,
then the join pattern $j$ can be matched at time $t$ using messages from
solution $\A$, yielding a smaller set of atoms $\A'$ and a substitution
function $\sigma$ which replaces closed expressions for the free variables in
$j$. The inference rules for single join pattern matching are given in Figure
\ref{fig:rule:join}.

\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\P'(j,\A) \leadsto \widehat\T_\J$}
\begin{equation*}
\inferrule[Join-FailEmpty]
{~}
{\P' (j, \emptyset) \leadsto fail}
\qquad
 \inferrule[Join-Match]
{
   \A = \Lbag \atm t:x<v> \Rbag \uplus \A'
}
{
  \P'(\atm x<y>, \A) \leadsto (t, (y \mapsto v), \A')
}
\end{equation*}
\begin{equation*}
\inferrule[Join-SubMatch]
{ \A = \Lbag t : \atm y<v> \Rbag \uplus \A'
\\ \P'(\atm x<y>, \A') \leadsto (t',\sigma,\A'')
}
{ \P'(\atm x<y>, \A) \leadsto (t', \sigma, \Lbag \atm t:y<v> \Rbag \uplus \A'' ) }
~~(x\not=y)
\end{equation*}
\begin{equation*}
\inferrule[Join-FailName]
{ \A = \Lbag \atm t:y<v> \Rbag \uplus \A'
\\ \P'(\atm x<y>, \A') \leadsto fail
}
{\P'(\atm x<y>, \A) \leadsto fail }
~~(x\not=y)
\end{equation*}
\end{minipage}}
\caption{Rules for matching single join patterns with failure detection.\label{fig:rule:join}}
\end{figure}

A rule $R = j_1~\&~j_2~\&~...~\&~j_n \stackrel{d}{\triangleright}Q$ is
executed when we can successively conclude $\P'(j,\A) \leadsto (t,
\sigma, \A')$ for all join patterns in $R$. When a rule $R$ can execute at time
$t$ in a solution of messages $\A$, we denote it by
\begin{equation*}
\boxed{\R_r(R,\A,t) \leadsto \widehat\T_r}
\end{equation*}
where the syntax of $\widehat\T_r$ can denote either success or failure:
\begin{align*}
\widehat\T_r ::={}& (P,\A')\\
    |\quad& fail
\end{align*}

This means that if we can conclude $\R_r(R,\A,t) \leadsto (P,\A')$, then the
rule $R$ can be executed in the message solution $\A$ at time $t$, yielding a
smaller set of messages $\A'$ and a new process $P$. The inference rules for
detecting reactions in a machine are given in Figure \ref{fig:rule:def}.

\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\R_r(R,\A,t) \leadsto \widehat\T_r$}
\begin{equation*}
\inferrule[Def-Match-Bottom]
{
   R = \atm x<\pi> \stackrel{d}{\triangleright} Q
\\ \P'(\atm x<\pi>, \A) \leadsto (t', \sigma, \A')
\\ t'+d \leq t }
{ \R_r(R,~\A,~t) \leadsto (\sigma Q, \A') }
\end{equation*}
\begin{equation*}
\inferrule[Def-Match]
{ R = (\atm x<\pi> ~\&~ J') \stackrel{d}{\triangleright} Q
\\ \P'(\atm x<\pi>, \A) \leadsto(t', \sigma, \A')
\\ t' + d \leq t
\\ \R_r(J' \stackrel{d}{\triangleright} Q, ~\A', ~t) \leadsto (Q', \A'') }
{ \R_r(R,A,t) \leadsto (\sigma Q', \A'') }
\end{equation*}
\begin{equation*}
\inferrule[Def-Fail-Late-Bottom]
{ R = \atm x<\pi> \stackrel{d}{\triangleright} Q
\\ \P'(\atm x<\pi>, \A) \leadsto (t',\sigma,\A')
\\ t'+d>t
}
{\R_r(R,~\A,~t) \leadsto fail}
\end{equation*}
\begin{equation*}
\inferrule[Def-Fail-Late]
{R = (\atm x<\pi> ~\&~ J') \stackrel{d}{\triangleright} Q
\\ R' = J \stackrel{d}{\triangleright} Q
\\ \P'(\atm x<\pi>,\A) \leadsto (t', \sigma, \A')
\\ t'+d>t
}
{
  \R_r(R,\A,t) \leadsto fail
}
\end{equation*}
\begin{equation*}
\inferrule[Def-Fail-Sub]
{R = (\atm x<\tilde\pi> ~\&~ J') \stackrel{d}{\triangleright} Q
\\ R' = J \stackrel{d}{\triangleright} Q
\\ \P'(\atm x<\tilde\pi>,\A) \leadsto (t', \sigma, \A')
\\ \R_r(R',\A',t) \leadsto fail
}
{
  \R_r(R,\A,t) \leadsto fail
}
\end{equation*}
\begin{equation*}
\inferrule[Def-Fail-NoMatch-Bottom]
{ R = \atm x<\tilde\pi> \stackrel{d}{\triangleright} Q
\\ \P'(\atm x<\tilde\pi>, \A) \leadsto fail
}
{\R_r(R,~\A,~t) \leadsto fail}
\end{equation*}
\begin{equation*}
\inferrule[Def-Fail-NoMatch]
{R = (\atm x<\tilde\pi> ~\&~ J') \stackrel{d}{\triangleright} Q
\\ \P'(\atm x<\tilde\pi>, \A) \leadsto fail
}
{
  \R_r(R,\A,t) \leadsto fail
}
\end{equation*}
\end{minipage}}
\caption{Rules for triggering reactions in a solution of messages, and at the
same time detecting whether a reaction is able to trigger or
not.\label{fig:rule:def}}
\end{figure}

%\clearpage

When checking if a machine $M$ can take an internal computational step, we
denote it by
\begin{equation*}
\boxed{\R_m(\M) \leadsto \widehat\M}
\end{equation*}
where the syntax of $\widehat\M$ denotes either the next machine state or quiescence:
\begin{align*}
 \widehat\M ::={}& \M' \\
   |\quad{}& quiescent
\end{align*}

Concluding $\R_m(\M) \leadsto \M'$ means that the machine $M$ can take a step
and become $\M'$, and concluding $\R_m(\M) \leadsto quiescent$ means that the
machine is \emph{stuck}, and can't execute any of its reaction rules at the
current instant. We have not proven it, but we hypothesise that for any
machine, the following theorems hold:
\begin{theorem}
$\R_m(\M) \leadsto quiescent \implies \forall \M'.~ \R_m(\M) \not\leadsto \M'$
\end{theorem}
\begin{theorem}
$\R_m(\M) \leadsto \M' \implies \R_m(\M) \not\leadsto quiescent$
\end{theorem}

The inference rules for machine internal computation is also generalised for
sets of machines, denoted by
\begin{equation*}
 \boxed{\R_m^*(\C) \leadsto \widehat\C}
\end{equation*}
where the syntax $\widehat\C$ can denote either quiescence or the next system
state:
\begin{align*}
 \widehat\C ::={}& \C' \\
    |\quad{}& quiescent
\end{align*}

The inference rules for machine internal computation are given in
Figure \ref{fig:rule:machine}.

\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\R_m(\M) \leadsto \widehat\M$}

\begin{equation*}
\inferrule[Machine-React]
{\M = \D \vdash_t^\phi \A
 \\ \D = \D' \cup \{ J \stackrel{d}{\triangleright} Q \}
 \\ \R_r(J \stackrel{d}{\triangleright} Q, \A, t) \leadsto (Q', \A')
 \\ \H(Q') \leadsto (\D^{new}, \A^{new})
}
{
 \R_m(\M) \leadsto (\D \cup \D^{new}) \vdash_t^\phi (\A' \uplus \A^{new})
}
\end{equation*}

% Redundant rule?

%\begin{equation*}
%\inferrule[Machine-React-Next]
%{\M = \D \vdash_t^\phi \A
% \\ \D = \D' \cup \{ J \stackrel{d}{\triangleright} Q \}
% \\ \R_r(J \stackrel{d}{\triangleright} Q, \A, t) \leadsto fail
% \\ \R_m(\D' \vdash_t^\phi \A) \leadsto \D^{new} \vdash_t^\phi \A^{new}}
%{ \R_m(\M) \leadsto (\D^{new} \cup \{ J \stackrel{d}{\triangleright} Q \}) \vdash_t^\phi \A^{new} }
%\end{equation*}

\begin{equation*}
\inferrule[Machine-Quiescent]
{\M = \D \vdash_t^\phi \A
 \\ \D = \D' \cup \{ J \stackrel{d}{\triangleright} Q \}
 \\ \R_r(J \stackrel{d}{\triangleright} Q, \A, t) \leadsto fail
 \\ \R_m(\D' \vdash_t^\phi \A) \leadsto quiescent
}
{ \R_m(\M) \leadsto quiescent }
\end{equation*}
\begin{equation*}
\inferrule[Machine-Empty-Quiescent]
{~}
{\R_m(\emptyset \vdash_t^\phi \A) \leadsto quiescent}
\end{equation*}

\doms{$\R_m^*(\C) \leadsto \widehat\C$}

\begin{equation*}
\inferrule[Machine-Rec-React]
{\C = \{\M\} \cup \C'
\\ \R_m(\M) \leadsto \M'
}
{\R_m^*(\C) \leadsto \{\M'\} \cup \C'}
\end{equation*}

\begin{equation*}
\inferrule[Machine-Rec-Quiescent]
{\C = \{\M'\} \cup \C'
\\ \R_m(\M) \leadsto quiescent
\\ \R_m^*(\C') \leadsto quiescent}
{\R_m^*(\C) \leadsto quiescent}
\end{equation*}

\begin{equation*}
\inferrule[Machine-Rec-Empty-Quiescent]
{~}
{\R_m^*(\emptyset) \leadsto quiescent}
\end{equation*}

\end{minipage}}
\caption{Rules for doing reactions in sets of machines, and at the same time
detecting whether they are quiescent or not.\label{fig:rule:machine}}
\end{figure}

%\clearpage

\subsection{Distribution}

Any machine $\D \vdash_t^\phi \A$ in the system can spawn new sublocations,
which will have the form $\D' \vdash_t^{\phi a} \A'$, i.e. they have a globally
unique name $a$ and their parent location string $\phi$ as a prefix of their
own. Every message in the new message solution $\A'$ will be delayed one time
unit relative to the current $t$.

Checking if a system $\C$ can spawn sublocations is denoted by
\begin{equation*}
\boxed{\F^*(\C) \leadsto \widehat\C}
\end{equation*}
We hypothesise that the following two theorems hold for any system $\C$:
\begin{theorem}
$\F^*(\C) \leadsto quiescent \implies \forall \C'.~ \F^*(\C) \not\leadsto \C'$
\end{theorem}
\begin{theorem}
$\F^*(\C) \leadsto \C' \implies \F^*(\C) \not\leadsto quiescent$
\end{theorem}
The inference rules for sublocation spawning are given in Figure
\ref{fig:rule:spawn}.

\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\F(\M) \leadsto \widehat\C$}

\begin{equation*}
\inferrule[Spawn-Machine]
{\M = \D \vdash_t^\phi \A
\\ \D = \D' \cup \{ a[D~\mathbf{in}~P] \}
\\ \H'(D) \leadsto \D_1^{new}
\\ \H((t+1):P) \leadsto (\D_2^{new}, \A^{new})
}
{ \F^*(\M) \leadsto \{ (\D' \vdash_t^\phi \A),~ (\D_1^{new} \cup \D_2^{new} \vdash_t^{\phi a} \A^{new}) \} }
\end{equation*}
\begin{equation*}
\inferrule[Spawn-Machine-Quiescent]
{\M = \D \vdash_t^\phi \A
\\ \D=\D' \cup \{ J \stackrel{d}{\triangleright} Q \}
\\ \F(\D' \vdash_t^\phi \A) \leadsto quiescent}
{ \F(\M) \leadsto quiescent }
\end{equation*}
\begin{equation*}
\inferrule[Spawn-Machine-Empty-Quiescent]
{\M = \emptyset \vdash_t^\phi \A}
{\F(\M) \leadsto quiescent}
\end{equation*}

\doms{$\F^*(\C) \leadsto \widehat\C$}
\begin{equation*}
\inferrule[Spawn]
{\C = \{\M\} \cup \C'
\\ \F^*(\M) \leadsto \C''
}
{\F^*(\C) \leadsto \C' \cup \C''}
\qquad
\inferrule[Spawn-Empty-Quiescent]
{~}
{\F^*(\emptyset) \leadsto quiescent}
\end{equation*}
\begin{equation*}
\inferrule[Spawn-Sub-Quiescent]
{\C = \{\M\} \cup \C'
\\ \F(\M) \leadsto quiescent
\\ \F^*(\C') \leadsto quiescent}
{\F^*(\C) \leadsto quiescent}
\end{equation*}
\end{minipage}}
\caption{Sublocation spawning rules}\label{fig:rule:spawn}
\end{figure}

If a machine contains a message with a port name defined on another machine in
the system, the message automatically gets transferred to that other machine,
enabling the machines to communicate.

Modelling that communication in our semantics proved to be a little involved,
and requires some auxiliary relations dividing message exchange into
two ``stages''; extraction and insertion.

Message extraction is denoted by
\begin{equation*}
\boxed{\E^*(\C) \leadsto (\C', \beta)}
\end{equation*}

If we can conclude $\E^*(\C) \leadsto (\C', \beta)$ for a system $\C$, then we
can extract a multiset of non-local messages $\beta$ from $\C$, yielding a new
system $\C'$ where the messages are removed from the respective machines. The
multiset $\beta$ has the form
\begin{equation*}
 \beta ::= \Lbag (a_1, P_1), (a_2, P_2), ..., (a_n, P_n) \Rbag
\end{equation*}
where $a_i$ is the unique name of the machine that the message $P_i$ was
extracted from. Inference rules for message extraction are given in Figure
\ref{fig:rule:extract}.

\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\E(\M) \leadsto (\M', \beta)$}
\begin{equation*}
\inferrule[Extract-Empty]
{\M = \D \vdash_t^\phi \emptyset}
{\E(\M) \leadsto (\M, \emptyset)}
\end{equation*}
\begin{equation*}
\inferrule[Extract-Nonlocal]
{\M = \D \vdash_t^{a \phi} \A' \uplus \Lbag \atm t:x<v> \Rbag
\\ \E(\D \vdash_t^{a \phi} \A') \leadsto (\M', \beta) }
{\E(\M) \leadsto (\M', \beta \uplus \Lbag (a, \atm x<v>) \Rbag)}
~~(x \not\in dv(\D))
\end{equation*}
\begin{equation*}
\inferrule[Extract-Local]
{\M = \D \vdash_t^\phi \A' \uplus \Lbag \atm t:x<v> \Rbag
\\\\ \E(\D \vdash_t^\phi \A') \leadsto (\D \vdash_t^\phi \A'', \beta)
}
{\E(\M) \leadsto (\D \vdash_t^\phi (\A'' \uplus \Lbag \atm t:x<v> \Rbag), \beta) }
~~(x \in dv(\D) \lor x = \mathit{go} \lor x = \mathit{halt})
\end{equation*}

\doms{$\E^*(\C) \leadsto (\C', \beta)$}
\begin{equation*}
\inferrule[Extract-Rec]
{ \C = \{\M\} \cup \C
\\ \E(\M) \leadsto (\M', \beta_1)
\\ \E^*(\C') \leadsto (\C'', \beta_2)
}
{ \E^*(\C) \leadsto (\{\M'\} \cup \C'', \beta_1 \uplus \beta_2) }
\end{equation*}
\begin{equation*}
\inferrule[Extract-Rec-Empty]
{~}
{\E^*(\emptyset) \leadsto (\emptyset, \emptyset)}
\end{equation*}
\end{minipage}}
\caption{Rules for extracting non-local messages from machines.}\label{fig:rule:extract}
\end{figure}

%\clearpage

Message insertion is denoted by
\begin{equation*}
\boxed{\I^*(\C, \beta) \leadsto (\C', \beta')}
\end{equation*}

If we can conclude $\I^*(\C, \beta) \leadsto (\C', \beta')$ for a system $\C$ and
a set of incoming messages $\beta$, we can insert the messages from $\beta$
into the respective receiver machines in $\C$, yielding a new system state
$\C'$ and a set of messages $\beta'$ that had no receivers. Messages are
inserted in the current instant of the machine. The rules for message insertion
are given in Figure \ref{fig:rule:insert}.

\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\I(\M, \beta) \leadsto (\M', \beta')$}
\begin{equation*}
\inferrule[Insert-Local]
{\M = \D \vdash_t^{b \phi} \A
\\\\ \beta = \beta' \uplus \Lbag (a, \atm x<v>) \Rbag
\\ \I(\M, \beta') \leadsto (\D \vdash_t^{\phi b} \A', \beta'')
}
{\I(\M, \beta) \leadsto (\D \vdash_t^{b \phi} (\A' \uplus \Lbag \atm (t+1):x<v> \Rbag), \beta'') }
~~(x \in dv(\D) \land a \linkUp{t} b)
\end{equation*}

\begin{equation*}
\inferrule[Insert-Lost]
{\M = \D \vdash_t^{b \phi} \A
\\\\ \beta = \beta' \uplus \Lbag (a, \atm x<v>) \Rbag
\\ \I(\M, \beta') \leadsto (\D \vdash_t^{b \phi} \A', \beta'')
}
{\I(\M, \beta) \leadsto (\D \vdash_t^{b \phi} \A', \beta'') }
~~(x \in dv(\D) \land \neg (a \linkUp{t} b))
\end{equation*}

\begin{equation*}
\inferrule[Insert-Nonlocal]
{\M = \D \vdash_t^{b \phi} \A
\\\\ \beta = \beta' \uplus \Lbag (a, \atm x<v>) \Rbag
\\ \I(\M, \beta') \leadsto (\D \vdash_t^{b \phi} \A', \beta'')
}
{\I(\M, \beta) \leadsto (\D \vdash_t^{b \phi} \A', \beta'' \uplus \Lbag (a, \atm x<v>) \Rbag) }
~~(x \not\in dv(\D))
\end{equation*}

\begin{equation*}
\inferrule[Insert-Empty]
{~}
{\I(\M, \emptyset) \leadsto (\M, \emptyset)}
\end{equation*}

\doms{$\I^*(\C, \beta) \leadsto (\C', \beta')$}
\begin{equation*}
\inferrule[Insert-Rec-Empty]
{~}
{\I^*(\emptyset, \beta) \leadsto (\emptyset, \beta)}
\end{equation*}
\begin{equation*}
\inferrule[Insert-Rec]
{\C = \{\M\} \cup \C'
\\ \I(\M, \beta) \leadsto (\M', \beta')
\\ \I^*(\C', \beta') \leadsto (\C'', \beta'') }
{\I^*(\C, \beta) \leadsto (\{\M'\} \cup \C'', \beta'')}
\end{equation*}

\end{minipage}}
\caption{Rules for inserting external messages into machines.}\label{fig:rule:insert}
\end{figure}

%\clearpage

The combined action of sequentially extracting and inserting messages in a
system while also detecting quiescence is denoted by
\begin{equation*}
\boxed{\X(\C) \leadsto \widehat\C}
\end{equation*}
We hypothesise that the following is always true for any system $\C$:
\begin{theorem}
$\X(\C) \leadsto quiescent \implies \forall \C'.~ \X(\C) \not\leadsto \C'$
\end{theorem}
\begin{theorem}
$\X(\C) \leadsto \C' \implies \X(\C) \not\leadsto quiescent$
\end{theorem}
The inference rules for message exchange are given in Figure \ref{fig:rule:exchange}.

\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\X(\C) \leadsto \widehat\C$}
\begin{equation*}
\inferrule[Exchange-Quiescent]
{\E^*(\C) \leadsto (\C', \emptyset)}
{\X(\C) \leadsto quiescent}
\end{equation*}
\begin{equation*}
\inferrule[Exchange]
{\E^*(\C) \leadsto (\C', \beta)
\\\I^*(\C', \beta) \leadsto (\C'', \beta') }
{ \X(\C) \leadsto \C'' }
~~(\beta \not= \emptyset)
\end{equation*}
\end{minipage}}
\caption{Rules for exchanging messages between machines and detecting when all
messages have been delivered.}\label{fig:rule:exchange}
\end{figure}

\subsection{Mobility and failure}

In order to define the rules for mobility and failure, we need a mapping between
unique machine identifiers and their fully qualified location sequence. Such a
map is a finite partial function denoted by
\begin{align*}
\Sigma ::={}&
    \{[a_1~~\phi_1], [a_2~~\phi_2], ..., [a_n~~\phi_n]\}
\end{align*}

Looking up a value is denoted by
\begin{equation*}
 \Sigma(a) = \phi \qquad \textrm{where} \ \  \exists i.~a=a_i \land \phi = \phi_i \land
 (a_i, \phi_i) \in \Sigma
\end{equation*}

If $\Sigma$ is not defined for some $a$, we denote it by
\begin{equation*}
 \Sigma(a) = \bot \qquad \textrm{where} \ \  \neg\exists i.~a=a_i \land \phi = \phi_i \land
 (a_i, \phi_i) \in \Sigma
\end{equation*}

Two partial functions $\Sigma_1$ and $\Sigma_2$ can be merged, given that
$\forall a.~ \Sigma_1(a) = \phi_1 \land \Sigma_2(a) = \phi_2 \implies \phi_1 =
\phi_2$. We denote the union of two partial functions by
\begin{equation*}
 \Sigma' = \Sigma_1 \cup \Sigma_2
\end{equation*}

In order to perform migration of a single machine in the system, we need to
know the fully qualified path of every other machine. The $\Sigma$-map of the
fully qualified paths in the system $\C$ is related to the system by
\begin{equation*}
\boxed{\L(\C) \leadsto \Sigma}
\end{equation*}

The inference rules for location path extraction are given in Figure
\ref{fig:rule:makemap}. In these rules, the union operation is always
well-defined due to theorem \ref{thm:unique-loc}.

\begin{figure}
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\L(\C) \leadsto \Sigma$}
\begin{equation*}
\inferrule[Loc-Insert]
{\C = \{\D \vdash_t^{\phi a} \A\} \cup \C'
\\ \L(\C') \leadsto \Sigma'}
{\L(\C) \leadsto \{[a~~\phi a]\} \cup \Sigma' }
\qquad
\inferrule[Loc-Empty]
{~}
{\L(\emptyset) \leadsto \{\}}
\end{equation*}
\end{minipage}}
\caption{Rules for determining the full prefix of all location names in a
system.}\label{fig:rule:makemap}
\end{figure}


Given a $\Sigma$-map for a given system, we can detect ``go'' messages and
determine what to do. Determining whether a machine needs to be halted or
migrated is denoted by
\begin{equation*}
\boxed{\G(\Sigma, \M) \leadsto \widehat\T_\G}
\end{equation*}
Where the syntax $\widehat\T_\G$ denotes either quiescence, location halting or
migration:
\begin{align*}
  \widehat\T_\G ::={}& quiescent \\
       |\quad& halt_\phi \\
       |\quad& (\M', \phi \mapsto \psi)
\end{align*}

Concluding $\G(\Sigma, \M) \leadsto halt_\phi$ means that the machine $\M$ in a
system with location map $\Sigma$ will cause all machines with location prefix
$\phi$ (including itself) to halt. Concluding $\G(\Sigma,\M) \leadsto (\M', \phi
\mapsto \psi)$ means that the machine $\M$ will cause all machines with location
prefix $\phi$ (including itself) to change that prefix to $\psi$, and that $\M$
will generate a continuation, becoming $\M'$.

The above relation is also generalised to sets of machines, denoted by
\begin{equation*}
\boxed{\G^*(\C) \leadsto \widehat\T_\G^*}
\end{equation*}
With the syntax $\widehat\T_\G^*$ defined as
\begin{align*}
  \widehat\T_\G^* ::={}& quiescent \\
         |\quad& halt_\phi \\
         |\quad& (\C', \phi \mapsto \psi)
\end{align*}
The inference rules for ``go'' and ``halt'' detection are given in Figure
\ref{fig:rule:detect}.

\begin{figure}[!p]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\G(\Sigma, \M) \leadsto \widehat\T_\G$}
\doms{$\widehat\T_\G ::= quiescent ~|~ halt_\phi ~|~ (\M', \phi \mapsto \psi)$}

\begin{equation*}
\inferrule[Detect-Empty-Quiescent]
{\M = \D \vdash_t^\phi \emptyset
}
{\G(\Sigma, \M) \leadsto quiescent}
\end{equation*}

\begin{equation*}
\inferrule[Detect-Late-Quiescent]
{\M = \D \vdash_t^\phi \A' \uplus \Lbag \atm t':x<v> \Rbag
\\ t' \not= t
\\ \G(\Sigma, \D \vdash_t^\phi \A') \leadsto quiescent
}
{\G(\Sigma, \M) \leadsto quiescent}
\end{equation*}

\begin{equation*}
\inferrule[Detect-Nomatch-Quiescent]
{\M = \D \vdash_t^\phi \A' \uplus \Lbag \atm t':x<v> \Rbag
\\ x \not= \textit{go}
\\ x \not= \textit{halt}
\\ \G(\Sigma, \D \vdash_t^\phi \A') \leadsto quiescent
}
{\G(\Sigma, \M) \leadsto quiescent}
\end{equation*}

\begin{equation*}
\inferrule[Detect-Halt]
{\M = \D \vdash_t^\phi \A' \uplus \Lbag \atm t:\mathit{halt}<\unit> \Rbag
}
{\G(\Sigma, \M) \leadsto halt_\phi}
\end{equation*}

\begin{equation*}
\inferrule[Detect-Go-Halt]
{\M = \D \vdash_t^\phi \A' \uplus \Lbag \atm t:\mathit{go}<\kappa(a,c)> \Rbag
\\ \Sigma(a) = \bot
}
{\G(\Sigma, \M) \leadsto halt_\phi}
\end{equation*}

\begin{equation*}
\inferrule[Detect-Migrate]
{\M = \D \vdash_t^\phi \A' \uplus \Lbag \atm t:\mathit{go}<\kappa(a,c)> \Rbag
\\ \Sigma(a) = \psi
\\ \M' = \D \vdash_t^{\phi b} (\A' \uplus \Lbag \atm t:c<\unit> \Rbag)
}
{\G(\Sigma, \M) \leadsto (\M', \phi b \mapsto \psi b)}
\end{equation*}

\doms{$\G^*(\C) \leadsto \widehat\T_\G^*$}

\begin{equation*}
\inferrule[Detect-Rec-Empty-Quiescent]
{~}
{\G^*(\Sigma, \emptyset) \leadsto quiescent}
\end{equation*}

\begin{equation*}
\inferrule[Detect-Rec-Quiescent]
{\C = \{ \M \} \cup \C'
\\ \G(\Sigma, \M) \leadsto quiescent
\\ \G^*(\Sigma, \C') \leadsto quiescent
}
{\G^*(\Sigma, \C) \leadsto quiescent}
\end{equation*}

\begin{equation*}
\inferrule[Detect-Rec-Halt]
{\C = \{ \M \} \cup \C'
\\ \G(\Sigma, \M) \leadsto halt_\phi
}
{\G^*(\Sigma, \C) \leadsto halt_\phi}
\end{equation*}

\begin{equation*}
\inferrule[Detect-Rec-Go]
{\C = \{\M\} \cup \C'
\\ \G(\Sigma, \M) \leadsto (\M', \phi \mapsto \psi) }
{\G^*(\Sigma, \C) \leadsto ( \{\M'\} \cup \C', \phi \mapsto \psi) }
\end{equation*}

\end{minipage}}
\caption{Rules for detecting \emph{go} and \emph{halt}
atoms.}\label{fig:rule:detect}
\end{figure}
%\clearpage
\noindent
Rewriting location prefixes in a system is denoted by
\begin{equation*}
\boxed{\V(\C, \phi \mapsto \psi) \leadsto \C'}
\end{equation*}
Concluding $\V(\C, \phi \mapsto \psi) \leadsto \C'$ means that the system $\C'$
is the system $\C$ where every machine with prefix $\phi$ has prefix $\psi$.

Halting machines based on their location prefix is denoted by
\begin{equation*}
\boxed{\K(\phi, \C) \leadsto \C'}
\end{equation*}
Concluding $\K(\phi, \C) \leadsto \C'$ means that the system $\C'$ is the system
$\C$ where every machine with prefix $\phi$ have been removed.

The inference rules for location rewriting and halting are given in Figure
\ref{fig:rule:mig} and \ref{fig:rule:halt} respectively.

\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\V(\C, \phi \mapsto \psi) \leadsto \C'$}
\begin{equation*}
\inferrule[Mig-Nomatch\footnote{With side-condition that $\phi$ is not a prefix of $\phi'$.}]
{\C = \{ \D \vdash_t^{\phi'} \A \} \cup \C'
\\ \V(\C', \phi \mapsto \psi) \leadsto \C''}
{\V(\C, \phi \mapsto \psi) \leadsto \{ \D \vdash_t^{\phi'} \A \} \cup \C'' }
\end{equation*}
\begin{equation*}
\inferrule[Mig-Rewrite]
{ \C = \{ \D \vdash_t^{\phi\beta} \A \} \cup \C'
\\ \M' = \D \vdash_t^{\psi\beta} \A
\\ \V(\C', \phi \mapsto \psi) \leadsto \C''
}
{\V(\C, \phi \mapsto \psi) \leadsto \{\M'\} \cup \C''}
\end{equation*}
\begin{equation*}
\inferrule[Mig-Empty]
{~}
{\V(\emptyset, \phi \mapsto \psi) \leadsto \emptyset}
\end{equation*}

\end{minipage}}
\caption{Rules for migrating machines with identical location prefixes.}\label{fig:rule:mig}
\end{figure}

\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\K(\phi, \C) \leadsto \C'$}
\begin{equation*}
\inferrule[Halt]
{\C = \{\D \vdash_t^{\phi \beta} \A\} \cup \C'
\\ \K(\phi, \C') \leadsto \C''}
{\K(\phi, \C) \leadsto \C''}
\end{equation*}
\begin{equation*}
\inferrule[Halt-NoMatch\footnote{$\phi$ is not a prefix of $\psi$.}]
{
 \C = \{\D \vdash_t^\psi \A\} \cup \C'
 \\ \K(\phi, \C') \leadsto \C''
}
{ \K(\phi, \C) \leadsto \{ \D \vdash_t^\psi \A \} \cup \C'' }
\qquad
\inferrule[Halt-Empty]
{~}
{\K(\phi,\emptyset) \leadsto \emptyset}
\end{equation*}
\end{minipage}}
\caption{Rules for halting machines with a specific location prefix.}\label{fig:rule:halt}
\end{figure}

%\clearpage
We need to apply either the $\V$ or $\K$ relation based on the conclusion of
$\G^*$. Applying either relation or concluding quiescence for a system $\C$, is
denoted by
\begin{equation*}
\boxed{\G'(\C) \leadsto \widehat\C}
\end{equation*}
We hypothesise that the following is always true for any system $\C$:
\begin{theorem}
$\G'(\C) \leadsto quiescent \implies \forall \C'.~ \G'(\C) \not\leadsto \C'$
\end{theorem}
\begin{theorem}
$\G'(\C) \leadsto \C' \implies \G'(\C) \not\leadsto quiescent$
\end{theorem}
The inference rules for the above relation are given in Figure
\ref{fig:rule:mighalt}.

\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\G'(\C) \leadsto \widehat\C$}
\begin{equation*}
\inferrule[MigHalt-Quiescent]
{\L(\C) \leadsto \Sigma
\\ \G^*(\Sigma, \C) \leadsto quiescent}
{\G'(\C) \leadsto quiescent}
\end{equation*}

\begin{equation*}
\inferrule[MigHalt-Halt]
{\L(\C) \leadsto \Sigma
\\ \G^*(\Sigma, \C) \leadsto halt_\phi
\\ \K(\phi, \C) \leadsto \C'}
{\G'(\C) \leadsto \C'}
\end{equation*}

\begin{equation*}
\inferrule[MigHalt-Migrate]
{\L(\C) \leadsto \Sigma
\\ \G^*(\Sigma, \C) \leadsto (\C', \phi \mapsto \psi)
\\ \V(\C', \phi \mapsto \psi) \leadsto \C''}
{\G'(\C) \leadsto \C''}
\end{equation*}
\end{minipage}}
\caption{Rules for migration and failure.}\label{fig:rule:mighalt}
\end{figure}

Progression of time occurs when every other phase of the system is quiescent.
Progression is denoted by
\begin{equation*}
\boxed{\N(\C) \leadsto \C'}
\end{equation*}
Where $\N(\C) \leadsto \C'$ means that $\C'$ is the system $\C$ where the time
tag of every machine is incremented by $1$. The inference rules for time
progression are given in Figure \ref{fig:rule:progress}.


\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\N(\C) \leadsto \C'$}
\begin{equation*}
\inferrule[Progress]
{\C = \{ \D \vdash_t^\phi \A \} \cup \C'
\\ \M' =\D \vdash_{t+1}^\phi \A
\\ \N(\C') \leadsto \C'' }
{\N(\C) \leadsto \{\M'\} \cup \C''}
\end{equation*}

\begin{equation*}
\inferrule[Progress-Empty]
{~}
{\N(\emptyset) \leadsto \emptyset}
\end{equation*}
\end{minipage}}
\caption{Rules for the progression of time.}\label{fig:rule:progress}
\end{figure}

\subsection{Evaluation steps}

Relations defined up until now does not model how actual evaluation steps in the
system are executed. Taking an evaluation step in a system of machines composed
in parallel is denoted by
\begin{equation*}
\boxed{\C \longrightarrow \C'}
\end{equation*}
The inference rules for the evaluation relation are given in Figure
\ref{fig:rule:eval}.

We have defined five different ``phases'' of evaluation as judgement forms in
the semantics. For every phase except \textbf{Progress}, we can either conclude
quiescence or construct a new state of the system. If an evaluation phase yields
a new system state $\C'$, another evaluation phase that was quiescent for $\C$
may not necessarily also be quiescent for $\C'$. We define the five phases as
follows:

\begin{description}
 \item[React] Execution of reactions local to each machine in the system. The
 phase is quiescent when no rule in any machine can be satisfied by the set of
 messages on the machine at the given time instant. Evaluation in this phase
 does not maintain quiescence for any other phases. The phase is defined in
 terms of the $\R_m^*(\C) \leadsto \widehat\C$ relation in Figure
 \ref{fig:rule:machine}.

 \item[Spawn] Spawning of sublocations. The phase is quiescent when no machine
 has definitions of the form $a[D~\mathbf{in}~P]$. Evaluation in this phase
 maintains quiescence for \textbf{React}, \textbf{Comm} and \textbf{Halt/Go}.
 The phase is defined in terms of the $\F(\C) \leadsto \widehat\C$ relation in
 Figure \ref{fig:rule:spawn}.

 \item[Comm] Exchanges messages between machines in the system. Evaluation in
 this phase maintains quiescence for \textbf{React}, \textbf{Spawn} and
 \textbf{Halt/Go}. The phase is defined in terms of the $\X(\C) \leadsto
 \widehat\C$ relation in Figure \ref{fig:rule:exchange}.

 \item[Halt/Go] Handles migration and halting of machines. Evaluation in this
 phase maintains quiescence for \textbf{Spawn}. The phase is defined in terms of
 the $\G'(\C) \leadsto \widehat\C$ relation in Figure \ref{fig:rule:mighalt}.

 \item[Progress] Increments all time tags. Evaluation in this phase maintains
 quiescence for \textbf{Spawn}. The phase is defined in terms of the $\N(\C)
 \leadsto \C'$ relation Figure \ref{fig:rule:progress}.
\end{description}

The order in which we evaluate the different phases does not really matter, as
long as \textbf{Progress} is only applied when all other phases are quiescent.
However, an efficient implementation may be able to exploit knowledge about
which phases that maintain quiescence for other phases, to minimise the amount
of overhead involved with checking whether a given rule can be applied.

\begin{figure}[!ht]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\C \longrightarrow \C'$}
\begin{equation*}
\inferrule[Eval-Spawn]
{\F^*(\C) \leadsto \C'}
{\C \longrightarrow \C'}
\qquad
\inferrule[Eval-HaltGo]
{\G'(\C) \leadsto \C'}
{\C \longrightarrow \C'}
\qquad
\inferrule[Eval-Comm]
{\X(\C) \leadsto \C'}
{\C \longrightarrow \C'}
\qquad
\inferrule[Eval-React]
{\R_m^*(\C) \leadsto \C'}
{\C \longrightarrow \C'}
\end{equation*}

\begin{equation*}
\inferrule[Eval-Progress]
{\F^*(\C) \leadsto quiescent
\\ \G'(\C) \leadsto quiescent
\\ \R_m^*(\C) \leadsto quiescent
\\ \X(\C) \leadsto quiescent
\\ \N(\C) \leadsto \C'}
{\C \longrightarrow \C'}
\end{equation*}
\end{minipage}}
\caption{Rules for taking evaluation steps.}\label{fig:rule:eval}
\end{figure}
