% vim:spell:spelllang=en_gb:
\def\A{\mathcal{A}}
\def\C{\mathcal{C}}
\def\M{\mathcal{M}}
\def\D{\mathcal{D}}
\def\P{\mathcal{P}}
\def\S{\mathcal{S}}
\def\Q{\mathcal{Q}}
\def\atm#1<#2>{#1\left\langle#2\right\rangle}
\def\Go<#1>{\text{go}\left\langle#1\right\rangle}
\def\Halt<>{\text{halt}\langle\rangle}
\def\toJ#1{\stackrel{#1}{\triangleright}}
\def\inJ{~\mathbf{in}~}
\def\join#1in#2#3{#1\toJ{#2}#3}
\def\mscG#1#2{\vdash^{#1}_{#2}}
\def\mscJ{\mscG{t}{\phi}}
\def\timeJ#1{\stackrel{#1}{\propto}}
\def\defJ{~\mathbf{def}~}
\def\match{~\mathbf{match}~}
\def\with{~\mathbf{with}~}
\def\linkUp#1{\stackrel{#1}{\diamond}}
\def\migrates(#1,#2){\xLongrightarrow{#1\mapsto#2}}
\def\reduceable(#1){\text{reduceable}\left(#1\right)}
\def\quiet(#1){\text{quiet}\left(#1\right)}
\def\halted(#1){\text{halted}\left(#1\right)}
\def\doms#1{\fbox{\rule[-0.75em]{0pt}{2em}#1}}

In this chapter we will document our prototype language, Join.  The
language is very similar to the original core join-calculus in many
aspects, but has been extended with various syntactic and semantic
features. In the following sections, we will present each extension
and discuss our motivation for including it, as well as the possible
implications that it may have on the difficulty of implementing the
language for real-world applications.

At last we will document the final syntax and semantics of the Join
language language.

\section{The core calculus}
\fixme{move some of the stuff from the introduction here, since an introduction
should rather introduce the project and more abstract concepts than the
programming language, in my opinion.. RFC..}

\section{Pattern matching}
In the core calculus it is only possible to construct join patterns, that match
atom names, fully disregarding the data contents of the atoms. Thus, if one wishes
to have different reactions to take place for different types of content, one
has to use different atom names to signify this.

Consider for example these two similar implementations of a stack, drawn from \ref{MaMa2008AlgPat} :
\begin{verbatim}
def
     some<x> & push<y>  |> some<y:x>
  or empty<> & push<y>  |> some<y>
  or some<x> & pop<k>   |> match x with
                             x':[] -> k<x> & empty<>
                           | x':xs -> k<x> & some<xs>
  in empty<> & \ldots

def
    stack<x> & push<y>   |> stack<y:x>
    stack<x:xs> & pop<k> |> stack<xs> & k<x>
 in stack<[]> & \ldots
\end{verbatim}

Even superficial inspection reveals the latter to be preferable, as it is both
more compact, more readable and more intuitive to someone who has tried
programming with pattern matching like that of ML or Haskell.

Also, it simply seems as a natural extension of the core join calculus.

Because of this we have decided to include pattern matching on join patterns in our system.
The precise semantics of pattern matching can be found in Figure \ref{fig:rule:pat}.

\section{Distribution and mobility}
The model of distribution is based on the concept of \emph{locations}, which
are named collections of definitions and atoms, thus comprising separate
join-calculus machines. Besides this, locations are characterised by being able
to migrate from site to site by producing a special atom, $\Go<a,k>$, where $a$
is the name of the destination location and $\atm k <>$ is a continuation to be
triggered upon successful migration. Furthermore they are able to become inert
by producing the special atom, $\Halt<>$, in their context.

As a consequence, locations form a hierarchy or tree, that changes throughout
the execution of a join program as locations migrate and halt.
When a location decides to migrate or halt, the decision has consequences for
the entire branch of the location hierarchy, of which the migrating or halting
location is the root. Therefore, when a location migrates, the entire set of
sublocations also migrates, and halting similarly halts every sublocation.

\fixme{introduce the syntax of locations}

\section{Timing}

The core join-calculus has no notion of time, which makes it
impossible to reason about the behaviour of programs with real-time
constraints.  For instance, in distributed systems, it is common to
place a time constraint on external requests, to make sure that a
program don't wait for a response forever if a message should get
lost, or if the program in the other end crashes. To express a
constraint like that in the core join calculus would require that we
rely on a specific implementation being able to generate a message on
a given time interval:
\begin{align*}
  \textbf{def}\quad & \S k<x> ~|~ \S incall<> \triangleright P_{ok} \\
  \land\quad & \S timeout<> ~|~ \S incall<> \triangleright P_{error} \\
  \textbf{in}\quad & \S remotecall<k> ~|~ \S starttimer<timeout, 10>
                                      ~|~ \S incall<>
\end{align*}
In the example above, \emph{remotecall} is given $10$ time units to
return a result on the name \emph{k}. If \emph{k} makes it before the
time limit, the process $P_{ok}$ is started, otherwise the process
$P_{error}$ starts.  There is a problem with this approach though:
Even though we can provide a Join-implementation that does exactly as
described, there is no guarantee that the program will behave in a
similar way on different implementations.

The problem lies both in the non-deterministic choice between the two
reaction rules, and in the fact that messages are not required to be
processed in the same order as they arrive. Even if a result arrives
before the timeout fires, a valid implementation can choose to wait
for the \emph{timeout} message to appear, and consume that instead of
the \emph{k} message.

A possible solution to this problem is to extend the join-calculus
with a notion of time. Many other non-timed process calculi, including
CSP and CCS, has already been extended for this purpose. An overview
of some of the work that has been done in this area along with an
attempt to generalize some of the concepts of timed process calculi
has been presented in \cite{nicollin-overview}.

A timed extension also exists for join-calculus, called Timed Join
Calculus \cite{timed-join}. The calculus is extended with a model of
time using a \emph{discrete time domain}, where every process bears a
time tag denoting when it will be able to participate in a reaction.
The syntax is extended with a new tagging construction for processes,
and all reactions are tagged with a
\emph{delay} tag:
\begin{align*}
  P ::={} & ...    & D ::={}& J \stackrel{d}{\triangleright} P \\
          & t : P  &        & D \land D
\end{align*}
We can now model the example above without depending on special
messages that get captured by the environment:
\begin{align*}
  \textbf{def}\quad & \S k<x> ~|~ \S incall<> \triangleright P_{ok} \\
  \land\quad & \S incall<> \stackrel{16}{\triangleright} P_{error} \\
  \textbf{in}\quad & \S 0:remotecall<k> ~|~ \S 0:incall<>
\end{align*}
Here, we assume that the response message \emph{k} gets transferred
from an external location and gets time tagged as soon as it enters
the local solution.  The second reaction rule is tagged with a delay
of $16$ time units.  This means that every message on the left of
``$\triangleright$'' needs to be available for $16$ time units before
the reaction can happen, effectively allowing another reaction to
``steal'' messages in that time window. Rules with no tags implicitly
gets tagged with a delay of $0$. If the \emph{k} message therefore
arrives before the $16$ time units has passed, the first reaction can
take place immediately.


\subsection{Operational semantics}

\fixme{Is it necessary to repeat the operational semantics of the
article, or can we just refer to them?}


\subsection{Infinite instants}

The time domain in Timed Join Calculus is \emph{abstract}, where time
tags doesn't have any quantifiable correspondence with \emph{physical}
time. It is assumed that any computation takes zero time unless
delayed with a non-zero time tag, which is of course not a realistic
assumption, but as assumption that simplifies the model.
Alternatively, one could assume that any atomic computation took some
minimum amount of time, yielding a model that is closer to reality.
However, as argued in \cite{nicollin-overview}, this destroys the
generality of the model, as we will then tie the behaviour of programs
to an arbitrary assumption about execution speed.

Even if we did make very conservative choices for the minimum duration
of a single computation, this wouldn't make it possible to guarantee
that a given computation finishes in a well-defined time window: Since
a computer has finite computational resources, but can (in theory)
execute an arbitrary number of threads concurrently, the time a
computation takes isn't fixed. As an example, say that we choose that
the duration $\delta$ of a reaction in the CHAM is $1 ms$. For a
single process, this will enable us to guarantee that a computation
involving 20 reactions will finish in $20 ms$. But if we execute an
arbitrary number of instances of this process concurrently on the same
hardware, we can only expect that the time it takes for all the
processes to finish will be proportional to the number of processes.
The actual wall-clock time for a single time step therefore increases,
meaning our hypothetical (though conservative) choice is still too
low. If we increase the number of concurrent processes towards
infinity, even the most conservative choice for $\delta$ will result
in an assumption that we can execute an arbitrary number of reactions
in an instant. The result is a more complicated model with exactly the
same problems as the simpler model, where every computation is assumed
to be instantaneous.

Assuming that every atomic computation takes zero time can pose some
problems in the form of \emph{timelocks} (also called
\emph{Zeno-behaviour}). Since time can only progress when a
computation in an instant is done, a diverging computation can prevent
time from ever progressing, causing a global timelock.  In
\cite{timed-join} this is solved by adding a very restrictive type
system to the join calculus, which rejects all programs that aren't
guaranteed to let time progress. We have chosen not to study this type
system in detail, for several reasons: (1) Identifying all
non-timelocking programs is equivalent to solving the halting problem,
meaning that the set of accepted programs in the proposed type system
is a lot smaller than the actual set of valid programs, making it very
difficult to express useful behaviour. (2) The authors describe the
type system as a ``first attempt'' at solving the problem, and are yet
to prove the soundness of it.


\section{Language definition}

\subsection{Syntax}

The syntax of our prototype language is defined in figure \ref{fig:syntax}. In
this definition, $x,a$ range over names, $s$ range over strings of characters,
and $i,d,t \in \mathbb{N}_0$. All constructions marked with ``*'' is syntactic
sugar, and has equivalent encodings in the core Join language.

\begin{figure}
\newcommand{\alt}{\mid\kern-1pt\mid}
\begin{align*}
P,Q\quad::={}&             && \textbf{processes} \\
          & 0              && \quad\textrm{inert process} \\
 \alt\quad& x\langle e_1, e_2, ..., e_n \rangle
                           && \quad\textrm{asynchronous message} \\
 \alt\quad& P~\&~Q         && \quad\textrm{parallel composition} \\
 \alt\quad& \textbf{def}~D~\textbf{in}~P
                           && \quad\textrm{local definition} \\
 \alt\quad& t : P          && \quad\textrm{timed process} \\
 \alt\quad& \textbf{match}~e~\textbf{with}~\pi_1
               \rightarrow P_1 ~|~ ... ~|~ \pi_n \rightarrow P_n
                           && \quad\textrm{pattern matching} \\
 \alt\quad& \{ I_1; I_2; ...; I_n \}
                           && \quad\textrm{*instruction sequence} \\
D\quad ::={}&              && \textbf{join definitions} \\
         & J \stackrel{d}{\triangleright} P
                           && \quad\textrm{delayed reaction} \\
\alt\quad& D~\textbf{or}~D && \quad\textrm{disjunction} \\
\alt\quad& a[D~\textbf{in}~P] && \quad\textrm{sublocation} \\
J\quad ::={}&              && \textbf{join patterns}\\
         & x\langle \pi_1, \pi_2, ..., \pi_n \rangle
                           && \quad\textrm{message} \\
\alt\quad& x(\pi_1, \pi_2, ..., \pi_n)
                           && \quad\textrm{*synchronous message} \\
\alt\quad& J~\&~J          && \quad\textrm{synchronization} \\
\pi\quad ::={}&            && \textbf{algebraic patterns} \\
         & x               && \quad\textrm{variable} \\
\alt\quad& \kappa(\pi_1, \pi_2, ..., \pi_n)
                           && \quad\textrm{constructor pattern} \\
e\quad ::={}&              && \textbf{expressions} \\
         & i               && \quad\textrm{*integer} \\
\alt\quad& " s "         && \quad\textrm{*string} \\
\alt\quad& x               && \quad\textrm{variable} \\
\alt\quad& \kappa(e_1, e_2, ..., e_n)
                           && \quad\textrm{constructor expression} \\
e^\star\quad ::={}&         && \textbf{*sugared expressions} \\
         & i               && \quad\textrm{*integer} \\
\alt\quad& " s "         && \quad\textrm{*string} \\
\alt\quad& x               && \quad\textrm{*variable} \\
\alt\quad& \kappa(e^\star_1, e^\star_2, ..., e^\star_n)
                           && \quad\textrm{*constructor expression} \\
\alt\quad& x(e^\star_1, e^\star_2, ..., e^\star_ n)
                           && \quad\textrm{*synchronous call} \\
I\quad ::={}&              && \textbf{*instructions} \\
         & \mathbf{let}~\pi~=~e^\star
                           && \quad\textrm{*named values} \\
\alt\quad& \mathbf{run}~P  && \quad\textrm{*asynchronous process} \\
\alt\quad& \mathbf{do}~e^\star && \quad\textrm{*synchronous call} \\
\alt\quad& \textbf{match}~e^\star~\textbf{with}~\pi_1
               \rightarrow \{ I* \} ~|~ ... ~|~ \pi_n \rightarrow \{ I* \}
                           && \quad\textrm{*pattern match} \\
\alt\quad& \textbf{return}~e^\star~\textbf{to}~x
                           && \quad\textrm{*implicit continuation}
\end{align*}
\caption{Syntax of the Join language.\label{fig:syntax}}
\end{figure}


\subsection{Semantics}

We model the state of our system as a series of CHAMs composed in parallel. A
single CHAM is a four tuple $\M = (\D, \A, \phi, t)$ denoted by
\begin{equation*}
 \boxed{\M = \D \mscJ \A}
\end{equation*}
where $\D$ is a set of active definitions, $\A$ is a multiset
of present atoms, $\phi$ is an ordered sequence of location names, and $t$ is
a natural number denoting the current time. Multiple CHAMs are composed in
parallel using the operator $\parallel$. We denote one or more CHAMs composed
in parallel by
\begin{equation*}
 \boxed{\C = \M \Big\slash \C'\parallel\C''}
\end{equation*}

We also need a way to denote that a set of machines running in parallel are
synchronised on a global clock. We denote such a system with $\S_\C$, which
means that the parallel system $\C$ is synchronised. We denote synchronised and
unsynchronised systems with
\begin{equation*}
 \boxed{\mathcal{X} = \S_\C \Big\slash \C}
\end{equation*}

We begin by defining a structural equivalence between machines and parallel
compositions:
\begin{figure}[!h]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\C \equiv \C'$}
\begin{align*}
 \textsc{Eq-Sym}&&
 \inferrule
 {
    \C \equiv \C'
 }
 {
    \C' \equiv \C
 }\\
 \textsc{Eq-Trans}&&
 \inferrule
 {
    \C \equiv \C' \\ \C' \equiv \C''
 }
 {
    \C \equiv \C''
 }\\
 \textsc{Str-Assoc}&&
 (\C ~||~ \C) ~||~ \C''
    \equiv{}& \C ~||~ (\C ~||~ \C'')\\
 \textsc{Str-Com}&&
 \C~||~\C' \equiv{}& \C'~||~\C\\
 \textsc{Str-Ref}&&
 \C \equiv{}& \C\\
 \textsc{Str-Def}&&
 \D \vdash_{t}^\phi \A, (t' : \defJ D\inJ P)
    \equiv{}& \D \mscJ \A, (\defJ D \inJ t':P) \\
 \textsc{Str-Time}&&
 \D \vdash_{t}^\phi \A, (t' : (t'' : P))
    \equiv{}& \D \mscJ \A, (t'+t'') : P \\
 \textsc{Str-Par}&&
 \D \mscJ \A, (t' : (P_1 ~\&~ P_2))
    \equiv{}& \D \mscJ \A, (t':P_1), (t':P_2) \\
 \textsc{Str-Or}&&
 \D, D_1~\mathbf{or}~D_2 \mscJ \A
    \equiv{}& \D, D_1, D_2 \mscJ \A \\
\end{align*}
\end{minipage}}
\end{figure}

A process with its time tags removed is denoted by $P_{\star}$, defined by
\begin{align*}
  (t : x\langle \tilde u \rangle)_\star ={}& x\langle \tilde u \rangle \\
  (t : P)_\star ={}& P_\star \\
  (t : P_1~\&~P_2)_\star ={}& P_{1\star} ~\&~ P_{2\star}
\end{align*}

We say that a join pattern $J$ \emph{matches} a process $P$ when there exists a
$\sigma$ such that $\sigma J = P_\star$, where $\sigma$ is a function that
replaces variable names with closed expressions. We denote this with
$\mathcal{P}_J(J,P) = \sigma$. When a pattern $\pi$ matches a value $v$, we
denote it with $\mathcal{P}_\pi(\pi,v) = \sigma$. The rules for these two
judgement forms are given in the following table:
\begin{figure}[!h]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\mathcal{P}_J(J,P) = \sigma$} \doms{$\mathcal{P}_\pi(\pi, v) = \sigma$}
\begin{equation*}
\inferrule[Pat-Var]
{\pi = x}
{\mathcal{P}_\pi(\pi,v) = (x \mapsto v)}
\end{equation*}
\begin{equation*}
\inferrule[Pat-Cons]
{\pi = \kappa(\pi_1, \pi_2, ..., \pi_n)
\\ v = \kappa(v_1, v_2, ..., v_n)
\\ (\forall i\leq n.~\mathcal{P}_\pi(\pi_i,v_i) = \sigma_i)}
{
 \mathcal{P}_\pi(\pi,v) = \bigcup_{i\leq n} \mathcal{P}_\pi(\pi_i, v_i)
}
\end{equation*}
\begin{equation*}
\inferrule[Pat-Msg]
{P = x\langle v_1, v_2, ..., v_n \rangle
 \\ J = x\langle \pi_1, \pi_2, ..., \pi_n \rangle
 \\ (\forall i\leq n.~\mathcal{P}_\pi(\pi_i, v_i) = \sigma_i)}
{
 P_J(J,P) = \bigcup_{i\leq n} P_\pi(\pi_i, v_i)
}
\end{equation*}
\begin{equation*}
\inferrule[Pat-Par]
{J = J_1 ~\&~ J_2
\\ P = P_1 ~\&~ P_2
\\ \mathcal{P}_J(J_1,P_1) = \sigma_1
\\ \mathcal{P}_J(J_2,P_2) = \sigma_2
}
{\mathcal{P}_J(J,P) = \sigma_1 \cup \sigma_2}
\end{equation*}
\end{minipage}}
\caption{Inference rules for pattern matching.} \label{fig:rule:pat}
\end{figure}

When a machine, a parallel composition of machines, or a synchronised system of
machines can make an evaluation step, we denote it with
\begin{equation*}
 \boxed{\mathcal{X} \longrightarrow \mathcal{X}'}
\end{equation*}

Independent processes in a single machine can take evaluation steps
independently of each other, as well as independent machines can take local
steps without affecting each other.  we model this explicitly using the
auxilary rules in Figure \ref{fig:rule:aux}.
\begin{figure}[!h]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\M\longrightarrow\M'$}
\begin{align*}
\inferrule[Local-Red]
{ \mathcal{M} \equiv \mathcal{D}_1 \vdash_t^\phi \mathcal{A}_1
\\ \mathcal{M}' \equiv \mathcal{D}_1' \vdash_t^\phi \mathcal{A}_1'
\\ \mathcal{M} \longrightarrow \mathcal{M}'}
{ \mathcal{D}_2,\mathcal{D}_1 \vdash_t^\phi \mathcal{A}_1,\mathcal{A}_2
\longrightarrow \mathcal{D}_2,\mathcal{D}_1' \vdash_t^\phi \mathcal{A}_1',\mathcal{A}_2
}
\end{align*}
\begin{equation*}
\inferrule[Par-Red]
{\C_1 \equiv \M ~||~ \C_1' \\
 \C_2 \equiv \mathcal{M}' ~||~ \C_2' \\
 \M \longrightarrow \M'}
{\C_1 \longrightarrow \C_2}
\end{equation*}
\end{minipage}}
\caption{Auxilary rules to lighten the notation}\label{fig:rule:aux}
\end{figure}

We include a relaxed version of the time semantics from \cite{timed-join}. Our
version provides less guarantees about the order in which messages on the same
port is processed, and each CHAM is explicitly tagged with a natural number
that denotes the current time.

We begin by inductively defining a ternary relation $\cdot
\stackrel{\cdot}{\propto} \cdot$, which collects all the ready molecules in a
CHAM, and tags them with the time they became ready in Figure \ref{fig:rule:sched}.

\begin{figure}[!h]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\M\timeJ{t}P$}
\begin{equation*}
\inferrule[Sched-Msg]
{ \mathcal{M} \equiv \mathcal{D} \vdash_t^\phi \mathcal{A}', t' : x\langle \tilde u \rangle
\\ t' \leq t
}
{\mathcal{M} \stackrel{t'}{\propto} t':x\langle \tilde u \rangle}
\qquad
\inferrule[Sched-Par]
{\mathcal{M} \stackrel{t}{\propto} P
\\ \mathcal{M} \stackrel{t}{\propto} Q}
{ \mathcal{M} \stackrel{max(t,t')}{\propto} P~\&~Q }
\end{equation*}
\end{minipage}}
\caption{Inference rules relating machines, times and atoms, essentially extracting the delay part of the atoms.}
\label{fig:rule:sched}
\end{figure}

In the original join-calculus semantics, a two-way reduction relation called a
``heating/cooling relation'' is defined for doing scope extrusion and forking
off new locations. We only provide one-way reduction rules for this, namely \textsc{Red-Def} in Figure \ref{fig:rule:red}.
\begin{figure}[!h]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\C\longrightarrow\C'$}
\begin{equation*}
\inferrule[Red-Def]
{
\M \equiv~\vdash_t^\phi \mathbf{def}~D~\mathbf{in}~P
\\ \M' \equiv \sigma_{dv}D \vdash_t^\phi \sigma_{dv}P
}
{
  \M \longrightarrow \M'
}
\end{equation*}

\begin{equation*}
\inferrule[Red-Loc]
{
\M \equiv \D, a[ D \inJ P] \mscJ \A
\\ \C \equiv \D \mscJ \A ~||~ D \mscG{t}{\phi a} t : P
}
{
 \M \longrightarrow \C
}
\end{equation*}
\begin{equation*}
\inferrule[Red-React]
{
\M \equiv J \toJ{d} Q \mscJ P
\hva\\ \M' \equiv J \toJ{d} Q \mscJ t : \sigma_{rv}Q
\\ \M \timeJ{t'} P
\\ t'+d = t
\\ \P(P) = \sigma_{rv}J
}
{ \M \longrightarrow \M' }
\end{equation*}
\begin{align*}
\inferrule[Red-Match]
{\M \equiv~\vdash_t^\phi \textbf{match}~P~\textbf{with}~\pi_1
   \rightarrow Q_1 ~|~...~|~\pi_m \rightarrow Q_m
 \\ \M' \equiv~\vdash_t^\phi \sigma_{rv}Q_i
 \\ \mathcal{P}(P) = \sigma_{rv}\pi_i
 \\ \forall j < i.~\pi_j \not\preceq \sigma_{rv} \pi_i}
{\M \longrightarrow \M'}
\end{align*}
\end{minipage}}
\caption{Reduction rules}\label{fig:rule:red}
\end{figure}

To model unreliable communication between locations, we introduce the
$\diamond$ relation, which relates pairs of location names at specific times.
For example, $a \linkUp{t} b$ means that locations $a$ and $b$ can
communicate at time $t$. The relation is reflexive, because internal
communication of a machine is assumed to be reliable. Using this, we can
specify how the system behaves when the connection between two units disappear,
as declared by the rules in Figure \ref{fig:rule:coms}. Note that the rules
also state that communication can only take place when the time of the two
communicating machines are synchronised.
\begin{figure}[!h]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$a\linkUp{t}b$}
\begin{equation*}
\inferrule[Comm-Link]
{
   \C \equiv \D_1 \mscG{t}{a\phi} \A_1, \atm t:x<\tilde e > \parallel \D_2 \mscG{t}{b\psi} \A_2
\\ a \neq \Omega \neq b
\\ \C' \equiv \D_1 \mscG{t}{a\phi} \A_1 \parallel \D_2 \mscG{t}{b\psi} \A_2, \atm (t+1) : x <\tilde e>
\\ x \in dv(\D_2)
\\ a\linkUp{t} b
}
{ \C \longrightarrow \C' }
\end{equation*}
\begin{equation*}
\inferrule[Comm-Lost]
{
   \C \equiv \D_1 \mscG{t}{a\phi} \A_1, \atm t:x< \tilde e >\parallel \D_2 \mscG{t}{b\psi} \A_2
\\ \C' \equiv \D_1 \mscG{t}{a\phi} \A_1 \parallel \D_2 \mscG{t}{b\psi} \A_2
\\ x \in dv(\D_2)
\\ \neg(a \linkUp{t} b)}
{ \C ~\longrightarrow~ \C'}
\end{equation*}
\end{minipage}}
\caption{Inference rules for communication between machines.}\label{fig:rule:coms}
\end{figure}

In our system, time can only progress when no CHAM can take another step,
effectively synchronising all machines on a global clock. In a real setting,
this is of course not feasible, but it makes it easier to reason about
communicating devices in a simulated setting.

We model this by a binary relation, that serves as a marking of machines and
groups of machines as having reached a fixed point, in which time instant no
further reduction or communication may happen:

\[\boxed{\C\Rightarrow\C}\]

To be able to detect when a machine can't do any computations, we need to
ensure, that no reductions can be applied to a machine, and that all external
communication has been carried out. This is ensured by the
\textsc{Step-Machine} rule below.

When all machines have fulfilled the $\Rightarrow$ relation, the time of each machine is
incremented in one global step, as $\S_c$ is reduced to $\S_{c'}$, with $\S$ representing
the entire state of the interpreter.

\begin{figure}[!h]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\C\Rightarrow\C'$} \doms{$\mathcal{X} \longrightarrow \mathcal{X}'$} \doms{$\quiet(\M)$}
\begin{equation*}
\inferrule[Quiet-Machine]
{
\M \equiv \D \mscJ \A \\
\lnot\exists P' \in \A ~.~ \text{reduceable}[P'] \\
\forall P \in \A~.~ \Big( P\equiv t' : a\langle \tilde u  \rangle \\
                    \P(P)\in dv[\D]\Big)
\\ \forall D \in \D ~.~ \Big( D \equiv J\toJ{d}Q
                         \land \lnot\exists P' \subseteq\A~.~( \M \timeJ{t'} P'
                         \land t'+d\leq t
                         \land \P (P') \equiv \sigma_{dv}J)\Big)
}
{
  \quiet(\M)
}
\end{equation*}\\
\begin{equation*}
\inferrule[Step-Machine]
{
\M \equiv \D \mscJ \A \\
\quiet(\M) \\
\lnot\exists (t:\Go<a,k>) \in \A \\
}
{
  \M \Rightarrow \D \mscG{t+1}{\phi} \A
}
\end{equation*}\\
\begin{equation*}
\inferrule[Step-Par]
{
  \C \equiv \M \parallel \C' \qquad \C' \Rightarrow \C'' \qquad \M \Rightarrow \M'
}
{
  \C \Rightarrow \M'
}
\qquad
\inferrule[Step-System]
{
  \C \Rightarrow \C'
}
{
  \S_c \rightarrow \S_{c'}
}
\end{equation*}
where $\reduceable(P)$ is true for
\begin{multline*}
\P(P)\in \{ \defJ D \inJ Q, \ a\left[ D \inJ Q\right], \\
\ \match e \with \pi_1 \to e_1~|~..~|~\pi_n \to e_n,\ \Halt<> \}
\end{multline*}
\end{minipage}}
\caption{Inference rules for the progression of time}\label{fig:rule:time-step}
\end{figure}

\subsubsection*{Mobility}
Special meaning is attributed to the messages $\Halt<>$ and
$\Go<a, k>$, and they are regarded as primitives of the
language, separate from normal atoms, though similar in syntax.


\begin{figure}[!h]
\fbox{\begin{minipage}{0.97\textwidth}
\doms{$\C\migrates(\phi,\psi)\C$}
\doms{$S_{c} \longrightarrow S_{c'}$}
\doms{$\halted(\M)$}
\begin{equation*}
\inferrule[Mig-Go]
{
  \C\equiv\M\parallel\M_{dest} \\
  \M\equiv\D\mscG{t}{\phi a}\A,t:\Go<b,k> \\
  \M_{dest}\equiv\D'\mscG{t}{\psi b}\A' \\
  \quiet(\M) \\
  \psi \neq \phi a \beta \\
  \lnot \halted(\M_{dest})
}
{
  \C\migrates(\phi a, \psi b a)\D\mscG{t}{\psi b a} \A, \atm k<> \parallel \M_{dest}
}
\end{equation*}

\begin{equation*}
\inferrule[Mig-Recurse]
{
  \C \equiv \M\parallel\C_1 \\
  \M \equiv \D\mscG{t}{\phi a} \A \\
  \C_1\migrates(\phi, \psi)\C_1'
}
{
  \C \migrates(\phi,\psi) \D\mscG{t}{\psi a}\A\parallel\C_1'
}
\end{equation*}

\begin{equation*}
\inferrule[Mig-DontCare]
{
  \C \equiv \M\parallel\C_1 \\
  \M \equiv \D\mscG{t}{\phi} \A \\
  \C_1\migrates(\theta, \psi)\C_1'
}
{
  \C \migrates(\theta,\psi) \M\parallel\C_1'
}
\qquad
\inferrule[Mig-Sys]
{
  \C\migrates(a,b)\C'
}
{
  \S_{\C}\longrightarrow \S_{\C'}
}
\end{equation*}

\begin{equation*}
\inferrule[Mig-Halt]
{
  \M\equiv\D\mscG{t}{\phi a}\A,t:\Halt<> \\
  \quiet(\M)
}
{
  \M\migrates(\phi a, \Omega a)\D\mscG{t}{\Omega a} \A
}\qquad
\inferrule[Halted-Msc]
{
  \C\equiv \D\mscG{t}{\phi}\A\parallel\C' \\
  \phi=\Omega\alpha
}
{
  \halted(\D\mscG{t}{\phi}\A)
}
\end{equation*}
\end{minipage}}
\caption{Inference rules for migration and failure.}
\end{figure}
